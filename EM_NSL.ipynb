{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EM_NSL.ipynb",
      "provenance": [],
      "mount_file_id": "1wwlkioMD7TPeHD684hQq6AT4NoXTj6ev",
      "authorship_tag": "ABX9TyMrV7fwk8IS0hTh8agJHyw/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lasitha-Jayawardana/IDS/blob/main/EM_NSL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QudD2RzUpDJr"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import itertools\r\n",
        "from scipy import linalg\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib as mpl\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.model_selection import ShuffleSplit\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.compose import make_column_transformer\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "import math\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "# To change scientific numbers to float\r\n",
        "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\r\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score,train_test_split\r\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix,f1_score\r\n",
        "from sklearn import preprocessing \r\n",
        "from sklearn.mixture import GaussianMixture\r\n",
        "\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUM4zgqK8ssa"
      },
      "source": [
        "def loadData():\n",
        "  global df_train\n",
        "  global df_test\n",
        "\n",
        "  df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IDS Project/NSL_Train.csv')\n",
        "  df_test =  pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IDS Project/NSL_Test.csv')\n",
        "\n",
        "  for col in ['protocol_type', 'service', 'flag', 'class']:\n",
        "    df_train[col] = df_train[col].astype('category')\n",
        "    df_test[col] = df_test[col].astype('category')\n",
        "\n",
        "  df_train=df_train.drop(columns='id')\n",
        "  df_test=df_test.drop(columns='id')\n",
        "  df_train['class'] = df_train['class'].map( {'normal':0, 'anomaly':1} )\n",
        "  df_test['class'] = df_test['class'].map( {'normal':0, 'anomaly':1} )\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0d9QY0AmQVE"
      },
      "source": [
        "def balanceClass():\r\n",
        "  global df_train\r\n",
        "  global df_test\r\n",
        "  # Class count\r\n",
        "  count_class_0, count_class_1 = df_train['class'].value_counts()\r\n",
        "\r\n",
        "  # Divide by class\r\n",
        "  df_class_1= df_train[df_train['class'] == 0]\r\n",
        "  df_class_0 = df_train[df_train['class'] == 1]\r\n",
        "      \r\n",
        "  df_class_0_under = df_class_0.sample(count_class_1)\r\n",
        "  df_train = pd.concat([df_class_0_under, df_class_1], axis=0)\r\n",
        "  df_train.reset_index(drop=True,inplace=True)\r\n",
        "  df_train.groupby('class')['class'].count()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eswWmlZ73YxG"
      },
      "source": [
        "def addOtherLabel():\r\n",
        "  global df_train\r\n",
        "  global df_test\r\n",
        "\r\n",
        "  otherlabel = 'Other_service'\r\n",
        "  others1 = df_train['service'].value_counts().index[30:]\r\n",
        "  # apply new category label\r\n",
        "  df_train['service'] = df_train['service'].cat.add_categories([otherlabel])\r\n",
        "  df_train['service'] = df_train['service'].replace(others1, otherlabel)\r\n",
        "\r\n",
        "  df_train['service'].replace({'pop_3':'Other_service','sunrpc':'Other_service',\r\n",
        "                              'link':'Other_service','name':'Other_service','echo':'Other_service',\r\n",
        "                              'netbios_ns':'Other_service'}, inplace = True)\r\n",
        "\r\n",
        "  others1 = df_test['service'].value_counts().index[30:]\r\n",
        "  # apply new category label\r\n",
        "  df_test['service'] = df_test['service'].cat.add_categories([otherlabel])\r\n",
        "  df_test['service'] = df_test['service'].replace(others1, otherlabel)\r\n",
        "\r\n",
        "  df_test['service'].replace({'pop_3':'Other_service','sunrpc':'Other_service',\r\n",
        "                              'link':'Other_service','name':'Other_service','echo':'Other_service',\r\n",
        "                              'netbios_ns':'Other_service'}, inplace = True)\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZmFMb9jnB-j"
      },
      "source": [
        "def removeDuplicate():\r\n",
        "  print('Duplicates train:',df_train.duplicated().sum())\r\n",
        "  df_train.drop_duplicates(keep='first',inplace=True)\r\n",
        "  print(df_train.duplicated().sum())\r\n",
        "\r\n",
        "  print('Duplicates test:',df_test.duplicated().sum())\r\n",
        "  df_test.drop_duplicates(keep='first',inplace=True)\r\n",
        "  print(df_test.duplicated().sum())\r\n",
        "\r\n",
        "  df_train.reset_index(drop=True,inplace=True)\r\n",
        "  df_test.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbqbssLkl47d"
      },
      "source": [
        "def splitData():\r\n",
        "  global X_train\r\n",
        "  global Y_train\r\n",
        "  global X_test\r\n",
        "  global Y_test\r\n",
        "  global df_train\r\n",
        "  global df_test\r\n",
        "  \r\n",
        "  limit1 = df_train.shape[1]-1\r\n",
        "      \r\n",
        "  X_train = df_train.iloc[:,0:limit1] # train set features\r\n",
        "  Y_train = df_train.iloc[:,limit1]\r\n",
        "      \r\n",
        "  limit2 = df_test.shape[1]-1\r\n",
        "      \r\n",
        "  X_test = df_test.iloc[:,0:limit2] # test set features\r\n",
        "  Y_test = df_test.iloc[:,limit2]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDfTOmOj2hXy"
      },
      "source": [
        "def labelEncoding(X_train,X_test):\r\n",
        "  label_encoder = preprocessing.LabelEncoder() \r\n",
        " \r\n",
        "# Encode labels in column 'species'. \r\n",
        "  X_train['protocol_type']= label_encoder.fit_transform(X_train['protocol_type' ]) \r\n",
        "  X_test['protocol_type']= label_encoder.transform(X_test['protocol_type' ]) \r\n",
        "  X_train['flag']= label_encoder.fit_transform(X_train['flag' ]) \r\n",
        "  X_test['flag']= label_encoder.transform(X_test['flag' ]) \r\n",
        "  X_train['service']= label_encoder.fit_transform(X_train['service' ]) \r\n",
        "  X_test['service']= label_encoder.transform(X_test['service' ]) \r\n",
        "  return X_train,X_test"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmGrs2Pxl-uE"
      },
      "source": [
        "def one_hot_encode(X_train,X_test):# Making categorical variables into numeric representation by one- hot encoding\r\n",
        "  \r\n",
        "  categorical_cols = ['protocol_type','flag','service']\r\n",
        "\r\n",
        "  # Training dataset one hot encoding\r\n",
        "  ohe = OneHotEncoder(handle_unknown = 'ignore')\r\n",
        "  ohe.fit(X_train[categorical_cols])\r\n",
        "  array_hot_encoded1 = ohe.transform(X_train[categorical_cols]).toarray()\r\n",
        "\r\n",
        "  data_hot_encoded1 = pd.DataFrame(array_hot_encoded1, index=X_train.index,columns=ohe.get_feature_names(categorical_cols))\r\n",
        "  X_train = X_train.drop(columns=categorical_cols)\r\n",
        "  X_train = pd.concat([data_hot_encoded1,X_train], axis=1)\r\n",
        "  \r\n",
        "  print('X_train shape :',X_train.shape)\r\n",
        "  \r\n",
        "  # Test dataset one hot encoding\r\n",
        "  array_hot_encoded2 = ohe.transform(X_test[categorical_cols]).toarray()\r\n",
        "  data_hot_encoded2 = pd.DataFrame(array_hot_encoded2, index=X_test.index,columns=ohe.get_feature_names(categorical_cols))\r\n",
        "  X_test = X_test.drop(columns=categorical_cols)\r\n",
        "  X_test = pd.concat([data_hot_encoded2,X_test], axis=1)\r\n",
        "  \r\n",
        "  print('X_test shape :',X_test.shape)\r\n",
        "  \r\n",
        "  return X_train, X_test"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67hGkTbPmA6J"
      },
      "source": [
        "def scaling(X_train,X_test,scaler):\r\n",
        "  \r\n",
        "  X_train = pd.DataFrame(scaler.fit_transform(X_train),columns=X_train.columns)\r\n",
        "  X_test = pd.DataFrame(scaler.transform(X_test),columns=X_test.columns)\r\n",
        "  return X_train,X_test\r\n",
        "\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI_cqvZbmFRQ"
      },
      "source": [
        "def runPCA(X_train,X_test,kPCA):\r\n",
        "  pca = PCA(n_components=kPCA, random_state = 453)\r\n",
        "  X_rtrain = pca.fit(X_train).transform(X_train)\r\n",
        "  X_rtest  = pca.transform(X_test)\r\n",
        "  return X_rtrain,X_rtest"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyvkPMWzKJZQ"
      },
      "source": [
        "def Gmm(X):\r\n",
        "  #check 1->4 components\r\n",
        "  tuned_parameters = {'n_components': np.array([1,2,3,4])}#,5,6,7,8,9,10\r\n",
        "  #construct grid search object that uses 2 fold cross validation\r\n",
        "  cv= [(slice(None), slice(None))]# for non crossvalidation\r\n",
        "  clf = GridSearchCV(GaussianMixture(),tuned_parameters,verbose=3,cv=cv,return_train_score=True,n_jobs=1)\r\n",
        "  #fit the data\r\n",
        "  clf.fit(X)\r\n",
        "  c = pd.DataFrame(clf.cv_results_)\r\n",
        "  print(c)\r\n",
        "  #plot the number of Gaussians against their rank\r\n",
        "  plt.scatter(clf.cv_results_['param_n_components'],\\\r\n",
        "              clf.cv_results_['rank_test_score'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bP7teMmTbpx"
      },
      "source": [
        "\r\n",
        "def searchGmm(X):\r\n",
        "  lowest_bic = np.infty\r\n",
        "  bic = []\r\n",
        "  n_components_range = range(3, 12)\r\n",
        "  cv_types = ['spherical', 'tied', 'diag', 'full']\r\n",
        "  for cv_type in cv_types:\r\n",
        "      for n_components in n_components_range:\r\n",
        "          # Fit a Gaussian mixture with EM\r\n",
        "          print (\"K = \",n_components)\r\n",
        "          gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type,init_params='kmeans',random_state=10002,verbose=0,verbose_interval=10,max_iter=1000)\r\n",
        "\r\n",
        "          gmm.fit(X)\r\n",
        "          bic.append(gmm.aic(X))\r\n",
        "          if bic[-1] < lowest_bic:\r\n",
        "              lowest_bic = bic[-1]\r\n",
        "              best_gmm = gmm\r\n",
        "\r\n",
        "  bic = np.array(bic)\r\n",
        "  color_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue',\r\n",
        "                                'darkorange'])\r\n",
        "  clf = best_gmm\r\n",
        "  bars = []\r\n",
        "  print(lowest_bic)\r\n",
        "  # Plot the BIC scores\r\n",
        "  plt.figure(figsize=(8, 6))\r\n",
        "  spl = plt.subplot(2, 1, 1)\r\n",
        "  for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\r\n",
        "      xpos = np.array(n_components_range) + .2 * (i - 2)\r\n",
        "      bars.append(plt.bar(xpos, bic[i * len(n_components_range):\r\n",
        "                                    (i + 1) * len(n_components_range)],\r\n",
        "                          width=.2, color=color))\r\n",
        "  plt.xticks(n_components_range)\r\n",
        "  plt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\r\n",
        "  plt.title('BIC score per model')\r\n",
        "  xpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\\r\n",
        "      .2 * np.floor(bic.argmin() / len(n_components_range))\r\n",
        "  plt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)\r\n",
        "  spl.set_xlabel('Number of components')\r\n",
        "  spl.legend([b[0] for b in bars], cv_types)\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvGL1Qhr0X5v"
      },
      "source": [
        "def XtoN():\r\n",
        "  global X_trainN\r\n",
        "  global X_testN\r\n",
        "  global kmeans\r\n",
        "  X_trainN = X_train\r\n",
        "  X_testN = X_test\r\n",
        "  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdSZpFW30kQ9"
      },
      "source": [
        "# Running K means with multible seeds\r\n",
        " \r\n",
        "def searchSeed(X_train,no_of_clusters):\r\n",
        "  global kmeans\r\n",
        "  best_seed = None\r\n",
        "\r\n",
        "\r\n",
        "  min_inertia=0.0\r\n",
        "  \r\n",
        "  seeds = [0,9500,10000,10500,11000,15000,18000 , 20000, 40000, 60000, 80000,90000, 120000]\r\n",
        "  for seed in seeds:\r\n",
        "\r\n",
        "    \r\n",
        "    #print('\\n seed= {} \\n'.format(seed))\r\n",
        "        \r\n",
        "    kmeans = KMeans(n_clusters=no_of_clusters, init='k-means++',random_state=seed)\r\n",
        "    kmeans = kmeans.fit(X_train)\r\n",
        "    \r\n",
        "    inertia = kmeans.inertia_\r\n",
        "    #print(\"The innertia for : 2 Clusters is:\", inertia) \r\n",
        "    # if current measurement of heterogeneity is lower than previously seen,\r\n",
        "    # update the minimum record of heterogeneity.\r\n",
        "    if min_inertia == 0 :\r\n",
        "        min_inertia = inertia\r\n",
        "        best_seed = seed\r\n",
        "    if inertia < min_inertia:\r\n",
        "        min_inertia = inertia\r\n",
        "        best_seed = seed\r\n",
        "        \r\n",
        "\r\n",
        "  print(\"\\nMin inertia : \", min_inertia)\r\n",
        "  print(\"Best Seed : \",best_seed)\r\n",
        "  return best_seed\r\n",
        " "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hePq1p3yz4a6"
      },
      "source": [
        "def runKmeans(k,best_seed):\r\n",
        "  global kmeans\r\n",
        "  # Running K means on K clusters\r\n",
        "  kmeans = KMeans(n_clusters=k,init='k-means++', random_state=best_seed)\r\n",
        "  kmeans = kmeans.fit(X_trainN)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfq4dBda0GWW"
      },
      "source": [
        "def binaryaccuracyScore():\r\n",
        "  label0 = 1- kmeans.labels_\r\n",
        "  label1 = kmeans.labels_\r\n",
        "  test_label1 = kmeans.predict(X_testN)\r\n",
        "  test_label0 = 1 - test_label1\r\n",
        "\r\n",
        "  a = accuracy_score(Y_train,label0)*100\r\n",
        "  aa = accuracy_score(Y_train,label1)*100\r\n",
        "  if (a>= aa):\r\n",
        "    l = test_label0\r\n",
        "    print(\"Accuracy Train: \",a)\r\n",
        "  else:\r\n",
        "    l = test_label1\r\n",
        "    print(\"Accuracy Train: \",aa)\r\n",
        "\r\n",
        "  print(\"Accuracy Test: \",accuracy_score(Y_test,l)*100)\r\n",
        "\r\n",
        "  print(\"\\nTest set precision : {:.4f}\".format(precision_score(Y_test, l)))\r\n",
        "  print(\"Test set recall    : {:.4f}\".format(recall_score(Y_test, l)))\r\n",
        "  print(\"Test set F1-score  : {:.4f}\".format(f1_score(Y_test, l)))\r\n",
        "  \r\n",
        "  cm = confusion_matrix(Y_test,l,normalize= 'true')\r\n",
        "  print('True Positives  : {:.2f}'.format(cm[1][1]))\r\n",
        "  print('True Negatives  : {:.2f}'.format(cm[0][0]))\r\n",
        "  print('False Positives : {:.2f}'.format(cm[0][1]))\r\n",
        "  print('False Negatives : {:.2f}'.format(cm[1][0]))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7exhZAm50Ocn"
      },
      "source": [
        "def oneVsAllScore(n,Trainlabel,Testlabel): \r\n",
        "\r\n",
        "  def oneVsAll(number):\r\n",
        "    if (number ==nn):\r\n",
        "      return 0\r\n",
        "    else:\r\n",
        "      return 1\r\n",
        "  m=[]\r\n",
        "  \r\n",
        "  for i in range(0, n):\r\n",
        "    nn=i\r\n",
        "    m.append(accuracy_score(Y_train,np.array(list(map(oneVsAll, Trainlabel)))))\r\n",
        "    print(i,\"  \",accuracy_score(Y_train,np.array(list(map(oneVsAll, Trainlabel)))))\r\n",
        "\r\n",
        "  print(\"Accuracy Train: \",max(m)*100)\r\n",
        "  index =m.index(max(m))\r\n",
        "  nn=index\r\n",
        "  #print(nn)\r\n",
        "  a=np.array(list(map(oneVsAll, Testlabel)))\r\n",
        "  print(\"Accuracy Test: \",accuracy_score(Y_test,a)*100)\r\n",
        "  print(\"Test set precision : {:.4f}\".format(precision_score(Y_test, a)))\r\n",
        "  print(\"Test set recall    : {:.4f}\".format(recall_score(Y_test, a)))\r\n",
        "  print(\"Test set F1-score  : {:.4f}\".format(f1_score(Y_test, a)))\r\n",
        "  cm = confusion_matrix(Y_test,a,normalize= 'true')\r\n",
        "  print('True Positives  : {:.2f}'.format(cm[1][1]))\r\n",
        "  print('True Negatives  : {:.2f}'.format(cm[0][0]))\r\n",
        "  print('False Positives : {:.2f}'.format(cm[0][1]))\r\n",
        "  print('False Negatives : {:.2f}'.format(cm[1][0]))\r\n",
        " "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI2u4EIknLuY"
      },
      "source": [
        "def mapMethodScore(n):\r\n",
        "  \r\n",
        "  l0=[]\r\n",
        "  Trainlabel = kmeans.labels_\r\n",
        "  Testlabel = kmeans.predict(X_testN)\r\n",
        "  def lmap(num):\r\n",
        "    if (num in l0):\r\n",
        "      return 0\r\n",
        "    else:\r\n",
        "      return 1\r\n",
        "  \r\n",
        "  for i in range(0, n):\r\n",
        "\r\n",
        "    #print(\"Cluster \", i)\r\n",
        "    c = Y_train[Trainlabel==i]\r\n",
        "    if (len(c[c == 1])<len(c[c == 0])):\r\n",
        "      l0.append(i)\r\n",
        "      #print(\"add to normal \",i)\r\n",
        "    #print(\"intrusion : \",len(c[c == 1]))\r\n",
        "    #print(\"normal : \",len(c[c == 0]))\r\n",
        "\r\n",
        "  ltrain = np.array(list(map(lmap , Trainlabel)))\r\n",
        "  print(\"Accuracy Train: \",accuracy_score(Y_train,ltrain)*100)\r\n",
        "\r\n",
        "  ltest = np.array(list(map(lmap , Testlabel)))\r\n",
        "\r\n",
        "  print(\"Accuracy Test: \",accuracy_score(Y_test,ltest)*100)\r\n",
        "  print(\"Test set precision : {:.4f}\".format(precision_score(Y_test,ltest)))\r\n",
        "  print(\"Test set recall    : {:.4f}\".format(recall_score(Y_test,ltest)))\r\n",
        "  print(\"Test set F1-score  : {:.4f}\".format(f1_score(Y_test,ltest)))\r\n",
        "  cm = confusion_matrix(Y_test,ltest,normalize= 'true')\r\n",
        "  print('True Positives  : {:.2f}'.format(cm[1][1]))\r\n",
        "  print('True Negatives  : {:.2f}'.format(cm[0][0]))\r\n",
        "  print('False Positives : {:.2f}'.format(cm[0][1]))\r\n",
        "  print('False Negatives : {:.2f}'.format(cm[1][0]))\r\n",
        "\r\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EhmfZO0mufAd",
        "outputId": "549bfd9f-0c2c-4726-86f6-7907d39f83ef"
      },
      "source": [
        "tmean=None\r\n",
        "tvar=None\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "last=1 #test=1\r\n",
        "i_index=-1\r\n",
        "myScore=[]\r\n",
        "\r\n",
        "def acc_score(actual,prediction,method):\r\n",
        "  global last\r\n",
        "  global i_index\r\n",
        "  global myScore\r\n",
        "  m=[[],[],[],[],[]]\r\n",
        "  cls = -1\r\n",
        "  \r\n",
        "  n = len(np.unique(prediction))\r\n",
        "  \r\n",
        "  def oneVsAll(num):\r\n",
        "    if (num ==cls):\r\n",
        "      return 0\r\n",
        "    else:\r\n",
        "      return 1\r\n",
        "\r\n",
        "  if (last == 0):\r\n",
        "    #testset\r\n",
        "    print(\"in test\")\r\n",
        "    myScore.clear()\r\n",
        "    cls = i_index\r\n",
        "    print(\"Index--------\",i_index)\r\n",
        "    oneVtest =np.array(list(map(oneVsAll, prediction)))\r\n",
        "    \r\n",
        "    myScore.append(accuracy_score(actual,oneVtest)*100)\r\n",
        "    myScore.append(precision_score(actual,oneVtest))\r\n",
        "    myScore.append(recall_score(actual,oneVtest))\r\n",
        "    myScore.append(f1_score(actual,oneVtest))\r\n",
        "    myScore.append(roc_auc_score(actual,oneVtest))\r\n",
        "\r\n",
        "\r\n",
        "    last = 1\r\n",
        "    \r\n",
        "  else:\r\n",
        "    #trainset\r\n",
        "    myScore.clear()\r\n",
        "    print(\"in train\")\r\n",
        "    for i in range(0, n):\r\n",
        "      cls=i\r\n",
        "      oneVall=np.array(list(map(oneVsAll, prediction)))\r\n",
        "      print(\"accurcy---\",i,\"  \",accuracy_score(actual,oneVall)*100)\r\n",
        "      m[0].append(accuracy_score(actual,oneVall)*100)\r\n",
        "      m[1].append(precision_score(actual,oneVall))\r\n",
        "      m[2].append(recall_score(actual,oneVall))\r\n",
        "      m[3].append(f1_score(actual,oneVall))\r\n",
        "      m[4].append(roc_auc_score(actual,oneVall))\r\n",
        "      \r\n",
        "    \r\n",
        "    i_index = m[0].index(max(m[0]))  \r\n",
        "    print(\"Index--------\",i_index)\r\n",
        "    myScore.append(m[0][i_index])\r\n",
        "    myScore.append(m[1][i_index])\r\n",
        "    myScore.append(m[2][i_index])\r\n",
        "    myScore.append(m[3][i_index])\r\n",
        "    myScore.append(m[4][i_index])\r\n",
        "\r\n",
        "    last = 0\r\n",
        "    print(\"accurcy max---\",myScore[0])\r\n",
        "  return myScore[0]\r\n",
        "  \r\n",
        "    \r\n",
        "b=0  \r\n",
        "\r\n",
        "def prec_score(estimator,x,y):\r\n",
        "  global b\r\n",
        "  global tvar\r\n",
        "  global tmean\r\n",
        "  if (b == 0):\r\n",
        "    b=1\r\n",
        "    print(np.shape(x))\r\n",
        "    #print(\"step 2: \",x)\r\n",
        "    #tmean = estimator['scaler'].mean_\r\n",
        "    #tvar = estimator['scaler'].var_\r\n",
        "    tvar = estimator['scaler']\r\n",
        "    tmean = estimator.get_params()\r\n",
        "    print(np.shape(estimator['scaler'].mean_))\r\n",
        "  return myScore[1]\r\n",
        "\r\n",
        "def rec_score(estimator,x,y):\r\n",
        "  return myScore[2]\r\n",
        "\r\n",
        "def f1_s_score(estimator,x,y):\r\n",
        "  return myScore[3]\r\n",
        "\r\n",
        "def roc_score(estimator,x,y):\r\n",
        "  return myScore[4]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "loadData()\r\n",
        "\r\n",
        "\r\n",
        "#balanceClass()\r\n",
        "#print(df_train.groupby('label')['label'].count())\r\n",
        "#addOtherLabel()\r\n",
        "removeDuplicate()\r\n",
        "splitData()\r\n",
        "\r\n",
        "\r\n",
        "X_train, X_test = one_hot_encode(X_train,X_test)\r\n",
        "#X_train,X_test = labelEncoding(X_train,X_test)\r\n",
        "#X_train, X_test = scaling(X_train,X_test,StandardScaler())\r\n",
        "#print(\"step 1: \",X_train)\r\n",
        "#X_train,X_test = runPCA(X_train,X_test,15)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "scalers_to_test = [StandardScaler(), MinMaxScaler(),None]\r\n",
        "params = {'scaler': scalers_to_test,\r\n",
        "        #'reduce_dim__n_components': n_features_to_test,\\\r\n",
        "        'regressor__n_clusters': [2],\r\n",
        "        'regressor__random_state':[100]}\r\n",
        "\r\n",
        "pipe = Pipeline(steps=[('scaler', StandardScaler()),('regressor', KMeans())],verbose=1)\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "rf = KMeans()\r\n",
        "p = Pipeline(steps=[('scaler', StandardScaler()), ('svc', rf)],verbose=3)\r\n",
        "#parameters = {'svc__n_estimators': [10],\r\n",
        "#              'svc__max_features': ['auto','sqrt'], }\r\n",
        "gs_rf = GridSearchCV(p, parameters, cv=2, scoring=scoring,verbose=3,return_train_score=True,refit=False,n_jobs=1)\r\n",
        "\r\n",
        "parameters = {'svc__n_clusters': [2],\r\n",
        "              'svc__random_state': [100], }\r\n",
        "\"\"\"\r\n",
        "scoring = {'accuracy': make_scorer(acc_score,greater_is_better=True,method='acc'),\r\n",
        "           'prec': prec_score,\r\n",
        "           'rec': rec_score,\r\n",
        "           'f1_s': f1_s_score,\r\n",
        "          'roc': roc_score}          \r\n",
        "\r\n",
        "grid = GridSearchCV(pipe, cv=5, scoring=scoring,param_grid= params,return_train_score=True,refit=False,n_jobs=1, verbose=1)\r\n",
        "grid.fit(X_train,Y_train)\r\n",
        "\r\n",
        "c = pd.DataFrame(grid.cv_results_)\r\n",
        "c"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "method 1\n",
            "\n",
            "\n",
            "Duplicates train: 9\n",
            "0\n",
            "Duplicates test: 3\n",
            "0\n",
            "X_train shape : (125964, 122)\n",
            "X_test shape : (22541, 122)\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.3s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   3.1s\n",
            "in train\n",
            "accurcy--- 0    81.0701385305442\n",
            "accurcy--- 1    18.9298614694558\n",
            "Index-------- 0\n",
            "accurcy max--- 81.0701385305442\n",
            "(25193, 122)\n",
            "(122,)\n",
            "in test\n",
            "Index-------- 0\n",
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.3s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   3.4s\n",
            "in train\n",
            "accurcy--- 0    9.64156710197277\n",
            "accurcy--- 1    90.35843289802723\n",
            "Index-------- 1\n",
            "accurcy max--- 90.35843289802723\n",
            "in test\n",
            "Index-------- 1\n",
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.3s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   3.1s\n",
            "in train\n",
            "accurcy--- 0    9.605842892867066\n",
            "accurcy--- 1    90.39415710713293\n",
            "Index-------- 1\n",
            "accurcy max--- 90.39415710713293\n",
            "in test\n",
            "Index-------- 1\n",
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.3s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   2.9s\n",
            "in train\n",
            "accurcy--- 0    90.56087008295955\n",
            "accurcy--- 1    9.439129917040448\n",
            "Index-------- 0\n",
            "accurcy max--- 90.56087008295955\n",
            "in test\n",
            "Index-------- 0\n",
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.3s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   3.2s\n",
            "in train\n",
            "accurcy--- 0    90.48110511273421\n",
            "accurcy--- 1    9.518894887265798\n",
            "Index-------- 0\n",
            "accurcy max--- 90.48110511273421\n",
            "in test\n",
            "Index-------- 0\n",
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.1s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   3.4s\n",
            "in train\n",
            "accurcy--- 0    18.969555035128806\n",
            "accurcy--- 1    81.0304449648712\n",
            "Index-------- 1\n",
            "accurcy max--- 81.0304449648712\n",
            "in test\n",
            "Index-------- 1\n",
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.1s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   3.5s\n",
            "in train\n",
            "accurcy--- 0    19.09260508871512\n",
            "accurcy--- 1    80.90739491128488\n",
            "Index-------- 1\n",
            "accurcy max--- 80.90739491128488\n",
            "in test\n",
            "Index-------- 1\n",
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.1s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   3.6s\n",
            "in train\n",
            "accurcy--- 0    18.84650498154249\n",
            "accurcy--- 1    81.1534950184575\n",
            "Index-------- 1\n",
            "accurcy max--- 81.1534950184575\n",
            "in test\n",
            "Index-------- 1\n",
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.1s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   3.6s\n",
            "in train\n",
            "accurcy--- 0    81.213035366967\n",
            "accurcy--- 1    18.786964633032984\n",
            "Index-------- 0\n",
            "accurcy max--- 81.213035366967\n",
            "in test\n",
            "Index-------- 0\n",
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.1s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   3.6s\n",
            "in train\n",
            "accurcy--- 0    19.07351540171483\n",
            "accurcy--- 1    80.92648459828517\n",
            "Index-------- 1\n",
            "accurcy max--- 80.92648459828517\n",
            "in test\n",
            "Index-------- 1\n",
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.0s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   2.2s\n",
            "in train\n",
            "accurcy--- 0    53.387845830190926\n",
            "Index-------- 0\n",
            "accurcy max--- 53.387845830190926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "in test\n",
            "Index-------- 0\n",
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.0s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   2.1s\n",
            "in train\n",
            "accurcy--- 0    52.788472988528554\n",
            "accurcy--- 1    47.21152701147144\n",
            "Index-------- 0\n",
            "accurcy max--- 52.788472988528554\n",
            "in test\n",
            "Index-------- 0\n",
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.0s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   2.2s\n",
            "in train\n",
            "accurcy--- 0    53.737149208113365\n",
            "accurcy--- 1    46.262850791886635\n",
            "Index-------- 0\n",
            "accurcy max--- 53.737149208113365\n",
            "in test\n",
            "Index-------- 0\n",
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.0s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   2.3s\n",
            "in train\n",
            "accurcy--- 0    54.02691223752629\n",
            "Index-------- 0\n",
            "accurcy max--- 54.02691223752629\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "in test\n",
            "Index-------- 0\n",
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.0s\n",
            "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   2.1s\n",
            "in train\n",
            "accurcy--- 0    53.38599555414417\n",
            "accurcy--- 1    46.61400444585583\n",
            "Index-------- 0\n",
            "accurcy max--- 53.38599555414417\n",
            "in test\n",
            "Index-------- 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   59.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_regressor__n_clusters</th>\n",
              "      <th>param_regressor__random_state</th>\n",
              "      <th>param_scaler</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_accuracy</th>\n",
              "      <th>split1_test_accuracy</th>\n",
              "      <th>split2_test_accuracy</th>\n",
              "      <th>split3_test_accuracy</th>\n",
              "      <th>split4_test_accuracy</th>\n",
              "      <th>mean_test_accuracy</th>\n",
              "      <th>std_test_accuracy</th>\n",
              "      <th>rank_test_accuracy</th>\n",
              "      <th>split0_train_accuracy</th>\n",
              "      <th>split1_train_accuracy</th>\n",
              "      <th>split2_train_accuracy</th>\n",
              "      <th>split3_train_accuracy</th>\n",
              "      <th>split4_train_accuracy</th>\n",
              "      <th>mean_train_accuracy</th>\n",
              "      <th>std_train_accuracy</th>\n",
              "      <th>split0_test_prec</th>\n",
              "      <th>split1_test_prec</th>\n",
              "      <th>split2_test_prec</th>\n",
              "      <th>split3_test_prec</th>\n",
              "      <th>split4_test_prec</th>\n",
              "      <th>mean_test_prec</th>\n",
              "      <th>std_test_prec</th>\n",
              "      <th>rank_test_prec</th>\n",
              "      <th>split0_train_prec</th>\n",
              "      <th>split1_train_prec</th>\n",
              "      <th>split2_train_prec</th>\n",
              "      <th>split3_train_prec</th>\n",
              "      <th>split4_train_prec</th>\n",
              "      <th>mean_train_prec</th>\n",
              "      <th>std_train_prec</th>\n",
              "      <th>split0_test_rec</th>\n",
              "      <th>split1_test_rec</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_test_rec</th>\n",
              "      <th>std_test_rec</th>\n",
              "      <th>rank_test_rec</th>\n",
              "      <th>split0_train_rec</th>\n",
              "      <th>split1_train_rec</th>\n",
              "      <th>split2_train_rec</th>\n",
              "      <th>split3_train_rec</th>\n",
              "      <th>split4_train_rec</th>\n",
              "      <th>mean_train_rec</th>\n",
              "      <th>std_train_rec</th>\n",
              "      <th>split0_test_f1_s</th>\n",
              "      <th>split1_test_f1_s</th>\n",
              "      <th>split2_test_f1_s</th>\n",
              "      <th>split3_test_f1_s</th>\n",
              "      <th>split4_test_f1_s</th>\n",
              "      <th>mean_test_f1_s</th>\n",
              "      <th>std_test_f1_s</th>\n",
              "      <th>rank_test_f1_s</th>\n",
              "      <th>split0_train_f1_s</th>\n",
              "      <th>split1_train_f1_s</th>\n",
              "      <th>split2_train_f1_s</th>\n",
              "      <th>split3_train_f1_s</th>\n",
              "      <th>split4_train_f1_s</th>\n",
              "      <th>mean_train_f1_s</th>\n",
              "      <th>std_train_f1_s</th>\n",
              "      <th>split0_test_roc</th>\n",
              "      <th>split1_test_roc</th>\n",
              "      <th>split2_test_roc</th>\n",
              "      <th>split3_test_roc</th>\n",
              "      <th>split4_test_roc</th>\n",
              "      <th>mean_test_roc</th>\n",
              "      <th>std_test_roc</th>\n",
              "      <th>rank_test_roc</th>\n",
              "      <th>split0_train_roc</th>\n",
              "      <th>split1_train_roc</th>\n",
              "      <th>split2_train_roc</th>\n",
              "      <th>split3_train_roc</th>\n",
              "      <th>split4_train_roc</th>\n",
              "      <th>mean_train_roc</th>\n",
              "      <th>std_train_roc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.447710</td>\n",
              "      <td>0.163037</td>\n",
              "      <td>0.289316</td>\n",
              "      <td>0.006359</td>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'regressor__n_clusters': 2, 'regressor__rando...</td>\n",
              "      <td>81.070139</td>\n",
              "      <td>90.358433</td>\n",
              "      <td>90.394157</td>\n",
              "      <td>90.560870</td>\n",
              "      <td>90.481105</td>\n",
              "      <td>88.572941</td>\n",
              "      <td>3.752061</td>\n",
              "      <td>1</td>\n",
              "      <td>81.092775</td>\n",
              "      <td>90.442687</td>\n",
              "      <td>90.457572</td>\n",
              "      <td>90.393069</td>\n",
              "      <td>90.413012</td>\n",
              "      <td>88.559823</td>\n",
              "      <td>3.733592</td>\n",
              "      <td>0.997574</td>\n",
              "      <td>0.991792</td>\n",
              "      <td>0.991172</td>\n",
              "      <td>0.990932</td>\n",
              "      <td>0.991998</td>\n",
              "      <td>0.992694</td>\n",
              "      <td>0.002471</td>\n",
              "      <td>2</td>\n",
              "      <td>0.998674</td>\n",
              "      <td>0.990921</td>\n",
              "      <td>0.990956</td>\n",
              "      <td>0.991191</td>\n",
              "      <td>0.990899</td>\n",
              "      <td>0.992528</td>\n",
              "      <td>0.003075</td>\n",
              "      <td>0.595333</td>\n",
              "      <td>0.802455</td>\n",
              "      <td>...</td>\n",
              "      <td>0.760319</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>1</td>\n",
              "      <td>0.594351</td>\n",
              "      <td>0.801220</td>\n",
              "      <td>0.802576</td>\n",
              "      <td>0.801314</td>\n",
              "      <td>0.801267</td>\n",
              "      <td>0.760146</td>\n",
              "      <td>0.082899</td>\n",
              "      <td>0.745667</td>\n",
              "      <td>0.887133</td>\n",
              "      <td>0.885079</td>\n",
              "      <td>0.886524</td>\n",
              "      <td>0.887110</td>\n",
              "      <td>0.858303</td>\n",
              "      <td>0.056323</td>\n",
              "      <td>1</td>\n",
              "      <td>0.745202</td>\n",
              "      <td>0.886030</td>\n",
              "      <td>0.886873</td>\n",
              "      <td>0.886196</td>\n",
              "      <td>0.886050</td>\n",
              "      <td>0.858070</td>\n",
              "      <td>0.056435</td>\n",
              "      <td>0.797035</td>\n",
              "      <td>0.898257</td>\n",
              "      <td>0.896686</td>\n",
              "      <td>0.897888</td>\n",
              "      <td>0.898315</td>\n",
              "      <td>0.877636</td>\n",
              "      <td>0.040305</td>\n",
              "      <td>1</td>\n",
              "      <td>0.796832</td>\n",
              "      <td>0.897437</td>\n",
              "      <td>0.898092</td>\n",
              "      <td>0.897540</td>\n",
              "      <td>0.897433</td>\n",
              "      <td>0.877467</td>\n",
              "      <td>0.040318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.691041</td>\n",
              "      <td>0.070664</td>\n",
              "      <td>0.283828</td>\n",
              "      <td>0.003166</td>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>MinMaxScaler(copy=True, feature_range=(0, 1))</td>\n",
              "      <td>{'regressor__n_clusters': 2, 'regressor__rando...</td>\n",
              "      <td>81.030445</td>\n",
              "      <td>80.907395</td>\n",
              "      <td>81.153495</td>\n",
              "      <td>81.213035</td>\n",
              "      <td>80.926485</td>\n",
              "      <td>81.046171</td>\n",
              "      <td>0.120990</td>\n",
              "      <td>2</td>\n",
              "      <td>81.051096</td>\n",
              "      <td>81.080867</td>\n",
              "      <td>81.020333</td>\n",
              "      <td>81.005448</td>\n",
              "      <td>81.078077</td>\n",
              "      <td>81.047164</td>\n",
              "      <td>0.030223</td>\n",
              "      <td>0.996011</td>\n",
              "      <td>0.997333</td>\n",
              "      <td>0.997265</td>\n",
              "      <td>0.997530</td>\n",
              "      <td>0.996707</td>\n",
              "      <td>0.996969</td>\n",
              "      <td>0.000551</td>\n",
              "      <td>1</td>\n",
              "      <td>0.997244</td>\n",
              "      <td>0.996874</td>\n",
              "      <td>0.996930</td>\n",
              "      <td>0.996865</td>\n",
              "      <td>0.997069</td>\n",
              "      <td>0.996996</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>0.595419</td>\n",
              "      <td>0.597260</td>\n",
              "      <td>...</td>\n",
              "      <td>0.594519</td>\n",
              "      <td>0.001684</td>\n",
              "      <td>2</td>\n",
              "      <td>0.594309</td>\n",
              "      <td>0.593836</td>\n",
              "      <td>0.594592</td>\n",
              "      <td>0.594953</td>\n",
              "      <td>0.594983</td>\n",
              "      <td>0.594534</td>\n",
              "      <td>0.000428</td>\n",
              "      <td>0.745297</td>\n",
              "      <td>0.747108</td>\n",
              "      <td>0.744759</td>\n",
              "      <td>0.743677</td>\n",
              "      <td>0.743446</td>\n",
              "      <td>0.744857</td>\n",
              "      <td>0.001316</td>\n",
              "      <td>2</td>\n",
              "      <td>0.744770</td>\n",
              "      <td>0.744296</td>\n",
              "      <td>0.744905</td>\n",
              "      <td>0.745171</td>\n",
              "      <td>0.745251</td>\n",
              "      <td>0.744879</td>\n",
              "      <td>0.000339</td>\n",
              "      <td>0.796668</td>\n",
              "      <td>0.797915</td>\n",
              "      <td>0.796441</td>\n",
              "      <td>0.795784</td>\n",
              "      <td>0.795552</td>\n",
              "      <td>0.796472</td>\n",
              "      <td>0.000830</td>\n",
              "      <td>2</td>\n",
              "      <td>0.796440</td>\n",
              "      <td>0.796113</td>\n",
              "      <td>0.796497</td>\n",
              "      <td>0.796658</td>\n",
              "      <td>0.796731</td>\n",
              "      <td>0.796488</td>\n",
              "      <td>0.000215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.216619</td>\n",
              "      <td>0.057254</td>\n",
              "      <td>0.236367</td>\n",
              "      <td>0.049580</td>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>None</td>\n",
              "      <td>{'regressor__n_clusters': 2, 'regressor__rando...</td>\n",
              "      <td>53.387846</td>\n",
              "      <td>52.788473</td>\n",
              "      <td>53.737149</td>\n",
              "      <td>54.026912</td>\n",
              "      <td>53.385996</td>\n",
              "      <td>53.465275</td>\n",
              "      <td>0.414655</td>\n",
              "      <td>3</td>\n",
              "      <td>53.484633</td>\n",
              "      <td>53.634478</td>\n",
              "      <td>53.397307</td>\n",
              "      <td>53.324865</td>\n",
              "      <td>53.485095</td>\n",
              "      <td>53.465276</td>\n",
              "      <td>0.103665</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.489898</td>\n",
              "      <td>3</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000336</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500084</td>\n",
              "      <td>0.500043</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500043</td>\n",
              "      <td>0.500034</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>3</td>\n",
              "      <td>0.500043</td>\n",
              "      <td>0.500021</td>\n",
              "      <td>0.500032</td>\n",
              "      <td>0.500043</td>\n",
              "      <td>0.500032</td>\n",
              "      <td>0.500034</td>\n",
              "      <td>0.000008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 83 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_fit_time  std_fit_time  ...  mean_train_roc  std_train_roc\n",
              "0       3.447710      0.163037  ...        0.877467       0.040318\n",
              "1       3.691041      0.070664  ...        0.796488       0.000215\n",
              "2       2.216619      0.057254  ...        0.500034       0.000008\n",
              "\n",
              "[3 rows x 83 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aR06EIKwHj3",
        "outputId": "691275e0-3a90-4896-9594-b7c786c74f7b"
      },
      "source": [
        "\r\n",
        "tvar.mean_\r\n",
        "tmean"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'memory': None,\n",
              " 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
              " 'scaler__copy': True,\n",
              " 'scaler__with_mean': True,\n",
              " 'scaler__with_std': True,\n",
              " 'steps': [('scaler',\n",
              "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "  ('svc', KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
              "          n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
              "          random_state=100, tol=0.0001, verbose=0))],\n",
              " 'svc': KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
              "        n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
              "        random_state=100, tol=0.0001, verbose=0),\n",
              " 'svc__algorithm': 'auto',\n",
              " 'svc__copy_x': True,\n",
              " 'svc__init': 'k-means++',\n",
              " 'svc__max_iter': 300,\n",
              " 'svc__n_clusters': 2,\n",
              " 'svc__n_init': 10,\n",
              " 'svc__n_jobs': None,\n",
              " 'svc__precompute_distances': 'auto',\n",
              " 'svc__random_state': 100,\n",
              " 'svc__tol': 0.0001,\n",
              " 'svc__verbose': 0,\n",
              " 'verbose': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y3XNZPUvqbG"
      },
      "source": [
        "w = gs_rf.predict(X_test)\r\n",
        "accuracy_score(Y_test,w)\r\n",
        "input()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-byd_ZS0gL-I",
        "outputId": "68646ac4-4244-4ad1-b40c-c353aeea5c13"
      },
      "source": [
        "\r\n",
        "\r\n",
        "X, y = make_classification(random_state=0)\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())],verbose=3)\r\n",
        "# The pipeline can be used as any other estimator\r\n",
        "# and avoids leaking the test set into the train set\r\n",
        "pipe.fit(X_train, y_train)\r\n",
        "Pipeline(steps=[('scaler', StandardScaler()), ('svc', SVC())],memory=True)\r\n",
        "pipe.score(X_test, y_test)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.0s\n",
            "[Pipeline] ............... (step 2 of 2) Processing svc, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.88"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaHjoIlonafu",
        "outputId": "e2e6b612-3562-475d-e87b-bdda8f038bcc"
      },
      "source": [
        "pipe"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('svc',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCS8kGGFj8nw",
        "outputId": "0dd34405-605f-4820-ab54-a68cb360ea93"
      },
      "source": [
        "np.shape(pipe['scaler'].var_)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WNbOqukh2PXe",
        "outputId": "7c0d9133-a435-4b5f-8df0-a297e92033bc"
      },
      "source": [
        "print(\"method 1\\n\\n\")\r\n",
        "loadData()\r\n",
        "#df_train.columns\r\n",
        "#print(df_train.groupby('attack_cat')['attack_cat'].count())\r\n",
        "#balanceClass()\r\n",
        "#print(df_train.groupby('label')['label'].count())\r\n",
        "#addOtherLabel()\r\n",
        "removeDuplicate()\r\n",
        "splitData()\r\n",
        "def overall_average_score(actual,prediction,estimator):\r\n",
        "  print(\"acual\",np.shape(estimator))\r\n",
        "  print(\"acual\",np.shape(actual))\r\n",
        "  # print(\"acual\",actual[0])\r\n",
        "  cc =len( np.unique(actual))\r\n",
        "  print(\"unique acual\",cc)\r\n",
        "  print(\"predicted\",np.shape(prediction))\r\n",
        "  cc =len( np.unique(prediction))\r\n",
        "  print(\"unique preict\",cc)\r\n",
        "  # print(\"predicted\",prediction[0])\r\n",
        "  return -1\r\n",
        "grid_scorer = make_scorer(overall_average_score, greater_is_better=True,estimator=X_test)\r\n",
        "\r\n",
        "X_train, X_test = one_hot_encode(X_train,X_test)\r\n",
        "#X_train,X_test = labelEncoding(X_train,X_test)\r\n",
        "X_train, X_test = scaling(X_train,X_test,StandardScaler())\r\n",
        "#X_train,X_test = runPCA(X_train,X_test,15)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "x =pd.DataFrame(np.array([1,2,3,4,5,6,7,8,9,10])) \r\n",
        "y = pd.DataFrame( np.array([11,21,31,41,51,61,71,81,91,101]))\r\n",
        "#check 1->4 components\r\n",
        "tuned_parameters = {'n_components': np.array([1,2,3])}#,5,6,7,8,9,10\r\n",
        "#construct grid search object that uses 2 fold cross validation\r\n",
        "cv= [(slice(None), slice(None))]# for non crossvalidation\r\n",
        "cv1 = ShuffleSplit(test_size=0.20, n_splits=1, random_state=0)#only use 1 cvset as 0.2 of training set\r\n",
        "clf = GridSearchCV(GaussianMixture(),tuned_parameters,verbose=3,cv=cv1,scoring=grid_scorer,return_train_score=True,n_jobs=1)\r\n",
        "#fit the data\r\n",
        "clf.fit(X_train, Y_train)\r\n",
        "\r\n",
        "\r\n",
        "c = pd.DataFrame(clf.cv_results_)\r\n",
        "c\r\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "method 1\n",
            "\n",
            "\n",
            "Duplicates train: 9\n",
            "0\n",
            "Duplicates test: 3\n",
            "0\n",
            "X_train shape : (125964, 122)\n",
            "X_test shape : (22541, 122)\n",
            "Fitting 1 folds for each of 3 candidates, totalling 3 fits\n",
            "[CV] n_components=1 ..................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "acual (22541, 41)\n",
            "acual (25193,)\n",
            "unique acual 2\n",
            "predicted (25193,)\n",
            "unique preict 1\n",
            "acual (22541, 41)\n",
            "acual (100771,)\n",
            "unique acual 2\n",
            "predicted (100771,)\n",
            "unique preict 1\n",
            "[CV]  n_components=1, score=(train=-1.000, test=-1.000), total=   1.8s\n",
            "[CV] n_components=2 ..................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.0s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "acual (22541, 41)\n",
            "acual (25193,)\n",
            "unique acual 2\n",
            "predicted (25193,)\n",
            "unique preict 2\n",
            "acual (22541, 41)\n",
            "acual (100771,)\n",
            "unique acual 2\n",
            "predicted (100771,)\n",
            "unique preict 2\n",
            "[CV]  n_components=2, score=(train=-1.000, test=-1.000), total=  11.0s\n",
            "[CV] n_components=3 ..................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   13.4s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "acual (22541, 41)\n",
            "acual (25193,)\n",
            "unique acual 2\n",
            "predicted (25193,)\n",
            "unique preict 3\n",
            "acual (22541, 41)\n",
            "acual (100771,)\n",
            "unique acual 2\n",
            "predicted (100771,)\n",
            "unique preict 3\n",
            "[CV]  n_components=3, score=(train=-1.000, test=-1.000), total=   4.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   18.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_n_components</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_train_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "      <th>std_train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.742717</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.059488</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_components': 1}</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.848764</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.105110</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_components': 2}</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.218147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.157873</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>{'n_components': 3}</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_fit_time  std_fit_time  ...  mean_train_score  std_train_score\n",
              "0       1.742717           0.0  ...              -1.0              0.0\n",
              "1      10.848764           0.0  ...              -1.0              0.0\n",
              "2       4.218147           0.0  ...              -1.0              0.0\n",
              "\n",
              "[3 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhyVNlGU26iR",
        "outputId": "06ee68f3-34f8-4fd1-b0c9-30917096efc5"
      },
      "source": [
        "c = pd.DataFrame(clf.cv_results_)\r\n",
        "clf.score"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method BaseSearchCV.score of GridSearchCV(cv=4, error_score=nan,\n",
              "             estimator=GaussianMixture(covariance_type='full',\n",
              "                                       init_params='kmeans', max_iter=100,\n",
              "                                       means_init=None, n_components=1,\n",
              "                                       n_init=1, precisions_init=None,\n",
              "                                       random_state=None, reg_covar=1e-06,\n",
              "                                       tol=0.001, verbose=0,\n",
              "                                       verbose_interval=10, warm_start=False,\n",
              "                                       weights_init=None),\n",
              "             iid='deprecated', n_jobs=1,\n",
              "             param_grid={'n_components': array([1, 2, 3])},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
              "             scoring=None, verbose=2)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kyXSfvayzs8-",
        "outputId": "70618ca3-22ca-4d8a-df12-5be794bcb545"
      },
      "source": [
        "print(\"method 1\\n\\n\")\r\n",
        "loadData()\r\n",
        "#df_train.columns\r\n",
        "#print(df_train.groupby('attack_cat')['attack_cat'].count())\r\n",
        "#balanceClass()\r\n",
        "#print(df_train.groupby('label')['label'].count())\r\n",
        "#addOtherLabel()\r\n",
        "removeDuplicate()\r\n",
        "splitData()\r\n",
        "\r\n",
        "\r\n",
        "X_train, X_test = one_hot_encode(X_train,X_test)\r\n",
        "#X_train,X_test = labelEncoding(X_train,X_test)\r\n",
        "X_train, X_test = scaling(X_train,X_test,StandardScaler())\r\n",
        "#X_train,X_test = runPCA(X_train,X_test,15)\r\n",
        "\r\n",
        "Gmm(X_train)\r\n",
        "\r\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "method 1\n",
            "\n",
            "\n",
            "Duplicates train: 9\n",
            "0\n",
            "Duplicates test: 3\n",
            "0\n",
            "X_train shape : (125964, 122)\n",
            "X_test shape : (22541, 122)\n",
            "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
            "[CV] n_components=1 ..................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ................................... n_components=1, total=   1.3s\n",
            "[CV] n_components=1 ..................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ................................... n_components=1, total=   1.2s\n",
            "[CV] n_components=2 ..................................................\n",
            "[CV] ................................... n_components=2, total=   7.3s\n",
            "[CV] n_components=2 ..................................................\n",
            "[CV] ................................... n_components=2, total=   2.2s\n",
            "[CV] n_components=3 ..................................................\n",
            "[CV] ................................... n_components=3, total=  10.5s\n",
            "[CV] n_components=3 ..................................................\n",
            "[CV] ................................... n_components=3, total=   8.8s\n",
            "[CV] n_components=4 ..................................................\n",
            "[CV] ................................... n_components=4, total=  17.9s\n",
            "[CV] n_components=4 ..................................................\n",
            "[CV] ................................... n_components=4, total=  10.6s\n",
            "[CV] n_components=5 ..................................................\n",
            "[CV] ................................... n_components=5, total=  18.7s\n",
            "[CV] n_components=5 ..................................................\n",
            "[CV] ................................... n_components=5, total=  25.2s\n",
            "[CV] n_components=6 ..................................................\n",
            "[CV] ................................... n_components=6, total=  42.1s\n",
            "[CV] n_components=6 ..................................................\n",
            "[CV] ................................... n_components=6, total=  22.9s\n",
            "[CV] n_components=7 ..................................................\n",
            "[CV] ................................... n_components=7, total=  44.6s\n",
            "[CV] n_components=7 ..................................................\n",
            "[CV] ................................... n_components=7, total=   6.8s\n",
            "[CV] n_components=8 ..................................................\n",
            "[CV] ................................... n_components=8, total= 1.1min\n",
            "[CV] n_components=8 ..................................................\n",
            "[CV] ................................... n_components=8, total=  45.3s\n",
            "[CV] n_components=9 ..................................................\n",
            "[CV] ................................... n_components=9, total= 1.1min\n",
            "[CV] n_components=9 ..................................................\n",
            "[CV] ................................... n_components=9, total=  27.2s\n",
            "[CV] n_components=10 .................................................\n",
            "[CV] .................................. n_components=10, total=  55.8s\n",
            "[CV] n_components=10 .................................................\n",
            "[CV] .................................. n_components=10, total=  36.9s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  8.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "id": "TPSOBq_ah4Rh",
        "outputId": "2ed43ca0-9001-4abb-9055-7373e8be3946"
      },
      "source": [
        "print(\"method 1\\n\\n\")\r\n",
        "loadData()\r\n",
        "#df_train.columns\r\n",
        "#print(df_train.groupby('attack_cat')['attack_cat'].count())\r\n",
        "#balanceClass()\r\n",
        "#print(df_train.groupby('label')['label'].count())\r\n",
        "#addOtherLabel()\r\n",
        "removeDuplicate()\r\n",
        "splitData()\r\n",
        "\r\n",
        "\r\n",
        "X_train, X_test = one_hot_encode(X_train,X_test)\r\n",
        "#X_train,X_test = labelEncoding(X_train,X_test)\r\n",
        "X_train, X_test = scaling(X_train,X_test,StandardScaler())\r\n",
        "#X_train,X_test = runPCA(X_train,X_test,15)\r\n",
        "\r\n",
        "Gmm(X_train)\r\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "method 1\n",
            "\n",
            "\n",
            "Duplicates train: 9\n",
            "0\n",
            "Duplicates test: 3\n",
            "0\n",
            "X_train shape : (125964, 122)\n",
            "X_test shape : (22541, 122)\n",
            "Fitting 1 folds for each of 4 candidates, totalling 4 fits\n",
            "[CV] n_components=1 ..................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  n_components=1, score=(train=-106.936, test=-106.936), total=   2.3s\n",
            "[CV] n_components=2 ..................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.6s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  n_components=2, score=(train=-97.237, test=-97.237), total=   4.2s\n",
            "[CV] n_components=3 ..................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    7.4s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  n_components=3, score=(train=278.822, test=278.822), total=  14.2s\n",
            "[CV] n_components=4 ..................................................\n",
            "[CV]  n_components=4, score=(train=243.663, test=243.663), total=  18.5s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   42.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   mean_fit_time  std_fit_time  ...  mean_train_score  std_train_score\n",
            "0       2.031035           0.0  ...       -106.936359              0.0\n",
            "1       3.673278           0.0  ...        -97.237249              0.0\n",
            "2      13.381794           0.0  ...        278.822450              0.0\n",
            "3      17.469504           0.0  ...        243.662731              0.0\n",
            "\n",
            "[4 rows x 13 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PkedJgnlYqb"
      },
      "source": [
        "print(\"method 1\\n\\n\")\r\n",
        "loadData()\r\n",
        "#df_train.columns\r\n",
        "#print(df_train.groupby('attack_cat')['attack_cat'].count())\r\n",
        "#balanceClass()\r\n",
        "#print(df_train.groupby('label')['label'].count())\r\n",
        "#addOtherLabel()\r\n",
        "removeDuplicate()\r\n",
        "splitData()\r\n",
        "\r\n",
        "\r\n",
        "X_train, X_test = one_hot_encode(X_train,X_test)\r\n",
        "#X_train,X_test = labelEncoding(X_train,X_test)\r\n",
        "X_train, X_test = scaling(X_train,X_test,StandardScaler())\r\n",
        "#X_train,X_test = runPCA(X_train,X_test,15)\r\n",
        "\r\n",
        "searchGmm(X_train)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPmtvKFHnWU9"
      },
      "source": [
        "print(\"method 1\\n\\n\")\r\n",
        "loadData()\r\n",
        "#df_train.columns\r\n",
        "#print(df_train.groupby('attack_cat')['attack_cat'].count())\r\n",
        "#balanceClass()\r\n",
        "#print(df_train.groupby('label')['label'].count())\r\n",
        "#addOtherLabel()\r\n",
        "removeDuplicate()\r\n",
        "splitData()\r\n",
        "\r\n",
        "\r\n",
        "X_train, X_test = one_hot_encode(X_train,X_test)\r\n",
        "#X_train,X_test = labelEncoding(X_train,X_test)\r\n",
        "X_train, X_test = scaling(X_train,X_test,StandardScaler())\r\n",
        "#X_train,X_test = runPCA(X_train,X_test,15)\r\n",
        "\r\n",
        "searchGmm(X_train)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69j5TFkorI-M"
      },
      "source": [
        "\r\n",
        "print(\"method 1\\n\\n\")\r\n",
        "loadData()\r\n",
        "#df_train.columns\r\n",
        "#print(df_train.groupby('attack_cat')['attack_cat'].count())\r\n",
        "#balanceClass()\r\n",
        "#print(df_train.groupby('label')['label'].count())\r\n",
        "#addOtherLabel()\r\n",
        "removeDuplicate()\r\n",
        "splitData()\r\n",
        "\r\n",
        "\r\n",
        "X_train, X_test = one_hot_encode(X_train,X_test)\r\n",
        "#X_train,X_test = labelEncoding(X_train,X_test)\r\n",
        "X_train, X_test = scaling(X_train,X_test,StandardScaler())\r\n",
        "#X_train,X_test = runPCA(X_train,X_test,15)\r\n",
        "\r\n",
        "searchGmm(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}