{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EM/Kmeans_UNSW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "19zY50E6DgETu-xibZnSwNlP5xKQJJiYy",
      "authorship_tag": "ABX9TyNYuq1rsk6Li0LLCkptrvYK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lasitha-Jayawardana/IDS/blob/main/EM_Kmeans_UNSW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QudD2RzUpDJr"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import itertools\r\n",
        "from scipy import linalg\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib as mpl\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.compose import make_column_transformer\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "import math\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "# To change scientific numbers to float\r\n",
        "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\r\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score,train_test_split\r\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix,f1_score\r\n",
        "from sklearn import preprocessing \r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.mixture import GaussianMixture\r\n",
        "from sklearn.metrics import make_scorer\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUM4zgqK8ssa"
      },
      "source": [
        "def loadData():\n",
        "  global df_train\n",
        "  global df_test\n",
        "  \n",
        "  df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IDS Project//UNSW_NB15_training-set.csv')\n",
        "  df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IDS Project//UNSW_NB15_testing-set.csv')\n",
        "\n",
        "  for col in ['proto','service','state']:\n",
        "    df_train[col] = df_train[col].astype('category')\n",
        "    df_test[col] = df_test[col].astype('category')\n",
        "\n",
        "  df_train=df_train.drop(columns='id')\n",
        "  df_test=df_test.drop(columns='id')\n",
        "  df_test.drop(df_test[df_test['state'] == 'ACC'].index, inplace = True)\n",
        "  df_test.drop(df_test[df_test['state'] == 'CLO'].index, inplace = True)\n",
        "  \n",
        "  df_train = shuffle(df_train)\n",
        "  df_test = shuffle(df_test)\n",
        "  df_test.reset_index(drop=True,inplace=True) \n",
        "  df_train.reset_index(drop=True,inplace=True) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0d9QY0AmQVE"
      },
      "source": [
        "def balanceClass():\r\n",
        "  global df_train\r\n",
        "  global df_test\r\n",
        "  # Class count\r\n",
        "  count_class_0, count_class_1 = df_train['label'].value_counts()\r\n",
        "\r\n",
        "  # Divide by class\r\n",
        "  df_class_1= df_train[df_train['label'] == 0]\r\n",
        "  df_class_0 = df_train[df_train['label'] == 1]\r\n",
        "      \r\n",
        "  df_class_0_under = df_class_0.sample(count_class_1)\r\n",
        "  df_train = pd.concat([df_class_0_under, df_class_1], axis=0)\r\n",
        "  df_train.reset_index(drop=True,inplace=True)\r\n",
        "  df_train.groupby('label')['label'].count()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eswWmlZ73YxG"
      },
      "source": [
        "def addOtherLabel():\r\n",
        "  global df_train\r\n",
        "  global df_test\r\n",
        "  label1 = 'other_proto'\r\n",
        "  others2 = df_train['proto'].value_counts().index[7:]\r\n",
        "  df_train['proto'] = df_train['proto'].cat.add_categories([label1])\r\n",
        "  df_train['proto'] = df_train['proto'].replace(others2, label1)\r\n",
        "  \r\n",
        "  label2 = 'other_state'\r\n",
        "  others3 = df_train['state'].value_counts().index[4:]\r\n",
        "  df_train['state'] = df_train['state'].cat.add_categories([label2])\r\n",
        "  df_train['state'] = df_train['state'].replace(others3, label2)\r\n",
        "  \r\n",
        "  df_test.drop(df_test[df_test['state'] == 'ACC'].index, inplace = True)\r\n",
        "  df_test.drop(df_test[df_test['state'] == 'CLO'].index, inplace = True)\r\n",
        "   \r\n",
        "  others4 = df_test['proto'].value_counts().index[7:]\r\n",
        "  df_test['proto'] = df_test['proto'].cat.add_categories([label1])\r\n",
        "  df_test['proto'] = df_test['proto'].replace(others4, label1)\r\n",
        "  \r\n",
        "   \r\n",
        "  others5 = df_test['state'].value_counts().index[4:]\r\n",
        "  df_test['state'] = df_test['state'].cat.add_categories([label2])\r\n",
        "  df_test['state'] = df_test['state'].replace(others5, label2)\r\n",
        "  \r\n",
        "  \r\n",
        "  \r\n",
        "#addOtherLabel()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZmFMb9jnB-j"
      },
      "source": [
        "def removeDuplicate():\r\n",
        "  print('Duplicates train:',df_train.duplicated().sum())\r\n",
        "  df_train.drop_duplicates(keep='first',inplace=True)\r\n",
        "  print(df_train.duplicated().sum())\r\n",
        "\r\n",
        "  print('Duplicates test:',df_test.duplicated().sum())\r\n",
        "  df_test.drop_duplicates(keep='first',inplace=True)\r\n",
        "  print(df_test.duplicated().sum())\r\n",
        "\r\n",
        "  df_train.reset_index(drop=True,inplace=True)\r\n",
        "  df_test.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbqbssLkl47d"
      },
      "source": [
        "def splitData():\r\n",
        "  global X_train\r\n",
        "  global Y_train\r\n",
        "  global X_test\r\n",
        "  global Y_test\r\n",
        "  global df_train\r\n",
        "  global df_test\r\n",
        "  \r\n",
        "  limit1 = df_train.shape[1]-1\r\n",
        "      \r\n",
        "  X_train = df_train.iloc[:,0:limit1-1] # train set features\r\n",
        "  Y_train = df_train.iloc[:,limit1]\r\n",
        "      \r\n",
        "  limit2 = df_test.shape[1]-1\r\n",
        "      \r\n",
        "  X_test = df_test.iloc[:,0:limit2-1] # test set features\r\n",
        "  Y_test = df_test.iloc[:,limit2]\r\n",
        "  X_train.reset_index(drop=True,inplace=True)\r\n",
        "  Y_train.reset_index(drop=True,inplace=True)\r\n",
        "  X_test.reset_index(drop=True,inplace=True)\r\n",
        "  Y_test.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDfTOmOj2hXy"
      },
      "source": [
        "def labelEncoding(X_train,X_test):\r\n",
        "  label_encoder = preprocessing.LabelEncoder() \r\n",
        "  \r\n",
        "# Encode labels in column 'species'. \r\n",
        "  X_train['proto']= label_encoder.fit_transform(X_train['proto' ]) \r\n",
        "  X_test['proto']= label_encoder.transform(X_test['proto' ]) \r\n",
        "  X_train['state']= label_encoder.fit_transform(X_train['state' ]) \r\n",
        "  X_test['state']= label_encoder.transform(X_test['state' ]) \r\n",
        "  X_train['service']= label_encoder.fit_transform(X_train['service' ]) \r\n",
        "  X_test['service']= label_encoder.transform(X_test['service' ]) \r\n",
        "  return X_train,X_test\r\n",
        "\r\n",
        "#X_train,X_test = labelEncoding(X_train,X_test)\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmGrs2Pxl-uE"
      },
      "source": [
        "def one_hot_encode(X_train,X_test):\r\n",
        "    \r\n",
        "    categorical_cols = ['proto','service','state']\r\n",
        "    # Training dataset one hot encoding\r\n",
        "    ohe = OneHotEncoder(handle_unknown = 'ignore')\r\n",
        "    ohe.fit(X_train[categorical_cols])\r\n",
        "    array_hot_encoded1 = ohe.transform(X_train[categorical_cols]).toarray()\r\n",
        "\r\n",
        "    data_hot_encoded1 = pd.DataFrame(array_hot_encoded1, index=X_train.index,columns=ohe.get_feature_names(categorical_cols))\r\n",
        "    X_train = X_train.drop(columns=categorical_cols)\r\n",
        "    X_train = pd.concat([data_hot_encoded1,X_train], axis=1)\r\n",
        "    \r\n",
        "    print('X_train shape :',X_train.shape)\r\n",
        "    \r\n",
        "    # Test dataset one hot encoding\r\n",
        "    array_hot_encoded2 = ohe.transform(X_test[categorical_cols]).toarray()\r\n",
        "    data_hot_encoded2 = pd.DataFrame(array_hot_encoded2, index=X_test.index,columns=ohe.get_feature_names(categorical_cols))\r\n",
        "    X_test = X_test.drop(columns=categorical_cols)\r\n",
        "    X_test = pd.concat([data_hot_encoded2,X_test], axis=1)\r\n",
        "    \r\n",
        "    print('X_test shape :',X_test.shape)\r\n",
        "    X_train = pd.DataFrame(X_train)\r\n",
        "    X_test = pd.DataFrame(X_test)\r\n",
        "\r\n",
        "    Y_train.reset_index(drop=True,inplace=True)\r\n",
        "    Y_test.reset_index(drop=True,inplace=True)\r\n",
        "    return X_train, X_test\r\n",
        "\r\n",
        "#X_train, X_test = one_hot_encode(X_train,X_test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67hGkTbPmA6J"
      },
      "source": [
        "def scaling(X_train,X_test,scaler):\r\n",
        "  \r\n",
        "  X_train = pd.DataFrame(scaler.fit_transform(X_train),columns=X_train.columns)\r\n",
        "  X_test = pd.DataFrame(scaler.transform(X_test),columns=X_test.columns)\r\n",
        "  return X_train,X_test\r\n",
        "\r\n",
        "#X_train, X_test = scaling(X_train,X_test,StandardScaler())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI_cqvZbmFRQ"
      },
      "source": [
        "def runPCA(X_train,X_test,kPCA):\r\n",
        "  pca = PCA(n_components=kPCA, random_state = 453)\r\n",
        "  X_rtrain = pca.fit(X_train).transform(X_train)\r\n",
        "  X_rtest  = pca.transform(X_test)\r\n",
        "  return X_rtrain,X_rtest"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-64ue0Je0Gm"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bP7teMmTbpx"
      },
      "source": [
        "\r\n",
        "def searchGmm(X):\r\n",
        "  lowest_bic = np.infty\r\n",
        "  bic = []\r\n",
        "  n_components_range = range(8, 20)\r\n",
        "  cv_types = ['spherical', 'tied', 'diag', 'full']\r\n",
        "  for cv_type in cv_types:\r\n",
        "      for n_components in n_components_range:\r\n",
        "          # Fit a Gaussian mixture with EM\r\n",
        "          print (\"K = \",n_components)\r\n",
        "          gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type,init_params='kmeans',random_state=12,verbose=0,verbose_interval=10,max_iter=1000)\r\n",
        "\r\n",
        "          gmm.fit(X)\r\n",
        "          bic.append(gmm.bic(X))\r\n",
        "          if bic[-1] < lowest_bic:\r\n",
        "              lowest_bic = bic[-1]\r\n",
        "              best_gmm = gmm\r\n",
        "\r\n",
        "  bic = np.array(bic)\r\n",
        "  color_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue',\r\n",
        "                                'darkorange'])\r\n",
        "  clf = best_gmm\r\n",
        "  bars = []\r\n",
        "  print(lowest_bic)\r\n",
        "  # Plot the BIC scores\r\n",
        "  plt.figure(figsize=(8, 6))\r\n",
        "  spl = plt.subplot(2, 1, 1)\r\n",
        "  for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\r\n",
        "      xpos = np.array(n_components_range) + .2 * (i - 2)\r\n",
        "      bars.append(plt.bar(xpos, bic[i * len(n_components_range):\r\n",
        "                                    (i + 1) * len(n_components_range)],\r\n",
        "                          width=.2, color=color))\r\n",
        "  plt.xticks(n_components_range)\r\n",
        "  plt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\r\n",
        "  plt.title('BIC score per model')\r\n",
        "  xpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\\r\n",
        "      .2 * np.floor(bic.argmin() / len(n_components_range))\r\n",
        "  plt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)\r\n",
        "  spl.set_xlabel('Number of components')\r\n",
        "  spl.legend([b[0] for b in bars], cv_types)\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WME3KXyumiL0"
      },
      "source": [
        "def searchKmeans():\r\n",
        " \r\n",
        "\r\n",
        "  # Running K means with multible Ks\r\n",
        "  best_seed = None\r\n",
        "  \r\n",
        "\r\n",
        "  X_value = X_train\r\n",
        "  min_inertia = []\r\n",
        "  #X_value = newdata.values\r\n",
        "  seeds = [0, 20000, 40000, 60000, 80000, 100000, 120000]\r\n",
        "  for seed in seeds:\r\n",
        "    inertia = []\r\n",
        "    no_of_clusters = range(2,10)\r\n",
        "    print('\\n\\n seed= {} \\n'.format(seed))\r\n",
        "    for f in no_of_clusters:\r\n",
        "        kmeans = KMeans(n_clusters=f, init='k-means++',random_state=seed)\r\n",
        "        kmeans = kmeans.fit(X_value)\r\n",
        "        \r\n",
        "        u = kmeans.inertia_\r\n",
        "        inertia.append(u)\r\n",
        "        print(\"The innertia for :\", f, \"Clusters is:\", u) \r\n",
        "        # if current measurement of heterogeneity is lower than previously seen,\r\n",
        "        # update the minimum record of heterogeneity.\r\n",
        "    if len(min_inertia) == 0 :\r\n",
        "        min_inertia = inertia\r\n",
        "        \r\n",
        "    if min(inertia,default=0) < min(min_inertia,default=0):\r\n",
        "        min_inertia = inertia\r\n",
        "        best_seed = seed\r\n",
        "    \r\n",
        "    # Creating the scree plot for Intertia - elbow method\r\n",
        "  fig, (ax1) = plt.subplots(1, figsize=(16,6))\r\n",
        "  xx = np.arange(len(no_of_clusters))\r\n",
        "  ax1.plot(xx, min_inertia,linewidth=4)\r\n",
        "  ax1.set_xticks(xx)\r\n",
        "  ax1.set_xticklabels(no_of_clusters, rotation='vertical')\r\n",
        "  plt.xlabel('Number of clusters')\r\n",
        "  plt.ylabel('Inertia Score')\r\n",
        "  plt.title('Inertia Plot per k for Best Seed {}'.format(best_seed))\r\n",
        "\r\n",
        "  # between-a-point-and-a-line-in-2-d/\r\n",
        "  def calc_distance(x1, y1, a, b, c):\r\n",
        "    d = abs((a * x1 + b * y1 + c)) / (math.sqrt(a * a + b * b))\r\n",
        "    return d\r\n",
        "\r\n",
        "  a = inertia[0] - inertia[-1]\r\n",
        "  b = no_of_clusters[-1] - no_of_clusters[0]\r\n",
        "  c1 = no_of_clusters[0] * inertia[-1]\r\n",
        "  c2 = no_of_clusters[-1] * inertia[0]\r\n",
        "  c = c1 - c2\r\n",
        "\r\n",
        "  r = no_of_clusters[-1]-1\r\n",
        "  r\r\n",
        "  distance_of_points_from_line = []\r\n",
        "\r\n",
        "  for k in range(int(r)):\r\n",
        "    distance_of_points_from_line.append(\r\n",
        "        calc_distance(no_of_clusters[k], inertia[k], a, b, c))\r\n",
        "    \r\n",
        "  plt.plot(no_of_clusters, distance_of_points_from_line)\r\n",
        "  K =no_of_clusters[distance_of_points_from_line.index(max(distance_of_points_from_line))]\r\n",
        "  print(\"Best K value : {}\".format(K))\r\n",
        "\r\n",
        "  return  K,best_seed "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvGL1Qhr0X5v"
      },
      "source": [
        "def XtoN():\r\n",
        "  global X_trainN\r\n",
        "  global X_testN\r\n",
        "  global kmeans\r\n",
        "  X_trainN = X_train\r\n",
        "  X_testN = X_test\r\n",
        "  "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdSZpFW30kQ9"
      },
      "source": [
        "# Running K means with multible seeds\r\n",
        " \r\n",
        "def searchSeed(X_train,no_of_clusters):\r\n",
        "  global kmeans\r\n",
        "  best_seed = None\r\n",
        "\r\n",
        "\r\n",
        "  min_inertia=0.0\r\n",
        "  \r\n",
        "  seeds = [0,9500,10000,10500,11000,15000,18000 , 20000, 40000, 60000, 80000,90000, 120000]\r\n",
        "  for seed in seeds:\r\n",
        "\r\n",
        "    \r\n",
        "    #print('\\n seed= {} \\n'.format(seed))\r\n",
        "        \r\n",
        "    kmeans = KMeans(n_clusters=no_of_clusters, init='k-means++',random_state=seed)\r\n",
        "    kmeans = kmeans.fit(X_train)\r\n",
        "    \r\n",
        "    inertia = kmeans.inertia_\r\n",
        "    #print(\"The innertia for : 2 Clusters is:\", inertia) \r\n",
        "    # if current measurement of heterogeneity is lower than previously seen,\r\n",
        "    # update the minimum record of heterogeneity.\r\n",
        "    if min_inertia == 0 :\r\n",
        "        min_inertia = inertia\r\n",
        "        best_seed = seed\r\n",
        "    if inertia < min_inertia:\r\n",
        "        min_inertia = inertia\r\n",
        "        best_seed = seed\r\n",
        "        \r\n",
        "\r\n",
        "  print(\"\\nMin inertia : \", min_inertia)\r\n",
        "  print(\"Best Seed : \",best_seed)\r\n",
        "  return best_seed\r\n",
        " "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hePq1p3yz4a6"
      },
      "source": [
        "def runKmeans(k,best_seed):\r\n",
        "  global kmeans\r\n",
        "  # Running K means on K clusters\r\n",
        "  kmeans = KMeans(n_clusters=k,init='k-means++', random_state=best_seed)\r\n",
        "  kmeans = kmeans.fit(X_trainN)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfq4dBda0GWW"
      },
      "source": [
        "def binaryaccuracyScore():\r\n",
        "  label0 = 1- kmeans.labels_\r\n",
        "  label1 = kmeans.labels_\r\n",
        "  test_label1 = kmeans.predict(X_testN)\r\n",
        "  test_label0 = 1 - test_label1\r\n",
        "\r\n",
        "  a = accuracy_score(Y_train,label0)*100\r\n",
        "  aa = accuracy_score(Y_train,label1)*100\r\n",
        "  if (a>= aa):\r\n",
        "    l = test_label0\r\n",
        "    print(\"Accuracy Train: \",a)\r\n",
        "  else:\r\n",
        "    l = test_label1\r\n",
        "    print(\"Accuracy Train: \",aa)\r\n",
        "\r\n",
        "  print(\"Accuracy Test: \",accuracy_score(Y_test,l)*100)\r\n",
        "\r\n",
        "  print(\"\\nTest set precision : {:.4f}\".format(precision_score(Y_test, l)))\r\n",
        "  print(\"Test set recall    : {:.4f}\".format(recall_score(Y_test, l)))\r\n",
        "  print(\"Test set F1-score  : {:.4f}\".format(f1_score(Y_test, l)))\r\n",
        "  \r\n",
        "  cm = confusion_matrix(Y_test,l,normalize= 'true')\r\n",
        "  print('True Positives  : {:.2f}'.format(cm[1][1]))\r\n",
        "  print('True Negatives  : {:.2f}'.format(cm[0][0]))\r\n",
        "  print('False Positives : {:.2f}'.format(cm[0][1]))\r\n",
        "  print('False Negatives : {:.2f}'.format(cm[1][0]))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7exhZAm50Ocn"
      },
      "source": [
        "def oneVsAllScore(n,Trainlabel,Testlabel): \r\n",
        "\r\n",
        "  def oneVsAll(number):\r\n",
        "    if (number ==nn):\r\n",
        "      return 0\r\n",
        "    else:\r\n",
        "      return 1\r\n",
        "  m=[]\r\n",
        "  \r\n",
        "  for i in range(0, n):\r\n",
        "    nn=i\r\n",
        "    m.append(accuracy_score(Y_train,np.array(list(map(oneVsAll, Trainlabel)))))\r\n",
        "    print(i,\"  \",accuracy_score(Y_train,np.array(list(map(oneVsAll, Trainlabel)))))\r\n",
        "\r\n",
        "  print(\"Accuracy Train: \",max(m)*100)\r\n",
        "  index =m.index(max(m))\r\n",
        "  nn=index\r\n",
        "  #print(nn)\r\n",
        "  a=np.array(list(map(oneVsAll, Testlabel)))\r\n",
        "  print(\"Accuracy Test: \",accuracy_score(Y_test,a)*100)\r\n",
        "  print(\"Test set precision : {:.4f}\".format(precision_score(Y_test, a)))\r\n",
        "  print(\"Test set recall    : {:.4f}\".format(recall_score(Y_test, a)))\r\n",
        "  print(\"Test set F1-score  : {:.4f}\".format(f1_score(Y_test, a)))\r\n",
        "  print(\"Test set Roc       : {:.4f}\".format(roc_auc_score(Y_test, a)))\r\n",
        "  cm = confusion_matrix(Y_test,a,normalize= 'true')\r\n",
        "  print('True Positives  : {:.2f}'.format(cm[1][1]))\r\n",
        "  print('True Negatives  : {:.2f}'.format(cm[0][0]))\r\n",
        "  print('False Positives : {:.2f}'.format(cm[0][1]))\r\n",
        "  print('False Negatives : {:.2f}'.format(cm[1][0]))\r\n",
        " "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI2u4EIknLuY"
      },
      "source": [
        "def mapMethodScore(n):\r\n",
        "  \r\n",
        "  l0=[]\r\n",
        "  Trainlabel = kmeans.labels_\r\n",
        "  Testlabel = kmeans.predict(X_testN)\r\n",
        "  def lmap(num):\r\n",
        "    if (num in l0):\r\n",
        "      return 0\r\n",
        "    else:\r\n",
        "      return 1\r\n",
        "  \r\n",
        "  for i in range(0, n):\r\n",
        "\r\n",
        "    #print(\"Cluster \", i)\r\n",
        "    c = Y_train[Trainlabel==i]\r\n",
        "    if (len(c[c == 1])<len(c[c == 0])):\r\n",
        "      l0.append(i)\r\n",
        "      #print(\"add to normal \",i)\r\n",
        "    #print(\"intrusion : \",len(c[c == 1]))\r\n",
        "    #print(\"normal : \",len(c[c == 0]))\r\n",
        "\r\n",
        "  ltrain = np.array(list(map(lmap , Trainlabel)))\r\n",
        "  print(\"Accuracy Train: \",accuracy_score(Y_train,ltrain)*100)\r\n",
        "\r\n",
        "  ltest = np.array(list(map(lmap , Testlabel)))\r\n",
        "\r\n",
        "  print(\"Accuracy Test: \",accuracy_score(Y_test,ltest)*100)\r\n",
        "  print(\"Test set precision : {:.4f}\".format(precision_score(Y_test,ltest)))\r\n",
        "  print(\"Test set recall    : {:.4f}\".format(recall_score(Y_test,ltest)))\r\n",
        "  print(\"Test set F1-score  : {:.4f}\".format(f1_score(Y_test,ltest)))\r\n",
        "  cm = confusion_matrix(Y_test,ltest,normalize= 'true')\r\n",
        "  print('True Positives  : {:.2f}'.format(cm[1][1]))\r\n",
        "  print('True Negatives  : {:.2f}'.format(cm[0][0]))\r\n",
        "  print('False Positives : {:.2f}'.format(cm[0][1]))\r\n",
        "  print('False Negatives : {:.2f}'.format(cm[1][0]))\r\n",
        "\r\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBMWK4tF3lrb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92189972-1863-4317-b64b-3e9ed3819226"
      },
      "source": [
        "y_true = [1,0]\r\n",
        "y_pred = [1,1]\r\n",
        "print(recall_score(y_true, y_pred))\r\n",
        "print(precision_score(y_true, y_pred))\r\n",
        "print(roc_auc_score(y_true, y_pred))\r\n",
        "print(f1_score(y_true, y_pred))\r\n",
        "cm = confusion_matrix(y_true,y_pred,normalize= 'true')\r\n",
        "print('True Positives  : {:.2f}'.format(cm[1][1]))\r\n",
        "print('True Negatives  : {:.2f}'.format(cm[0][0]))\r\n",
        "print('False Positives : {:.2f}'.format(cm[0][1]))\r\n",
        "print('False Negatives : {:.2f}'.format(cm[1][0]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.5\n",
            "0.5\n",
            "0.6666666666666666\n",
            "True Positives  : 1.00\n",
            "True Negatives  : 0.00\n",
            "False Positives : 1.00\n",
            "False Negatives : 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKoZl49ZLi1H",
        "outputId": "099e4c0c-84b9-48f3-e238-e3a6c34a1a3a"
      },
      "source": [
        "#.......................................................................Kmeans.......................................................\r\n",
        "trainScore = True\r\n",
        "cvCount = 5\r\n",
        "\r\n",
        "last=1 #test=0\r\n",
        "i_index=-1\r\n",
        "myScore=[]\r\n",
        "iCount=0\r\n",
        "fitNum=0\r\n",
        "\r\n",
        "\r\n",
        "def acc_score(actual,prediction,method):\r\n",
        "  global last\r\n",
        "  global i_index\r\n",
        "  global myScore\r\n",
        "  global iCount\r\n",
        "  global fitNum\r\n",
        " \r\n",
        "\r\n",
        "  m=[[],[],[],[],[]]\r\n",
        "  cls = -1\r\n",
        "  \r\n",
        "  \r\n",
        "  def oneVsAll(num):\r\n",
        "    if (num ==cls):\r\n",
        "      return 0\r\n",
        "    else:\r\n",
        "      return 1\r\n",
        "\r\n",
        "  if (last == 0 and trainScore):\r\n",
        "     #trainset\r\n",
        "    print(\"in train\")\r\n",
        "    \r\n",
        "    myScore.clear()\r\n",
        "    cls = i_index\r\n",
        "    #print(\"Index--------\",i_index)\r\n",
        "    oneVtest =np.array(list(map(oneVsAll, prediction)))\r\n",
        "    myScore.append(accuracy_score(actual,oneVtest)*100)\r\n",
        "    myScore.append(precision_score(actual,oneVtest))\r\n",
        "    myScore.append(recall_score(actual,oneVtest))\r\n",
        "    myScore.append(f1_score(actual,oneVtest))\r\n",
        "    myScore.append(roc_auc_score(actual,oneVtest))\r\n",
        "\r\n",
        "    last = 1\r\n",
        "    \r\n",
        "  else:\r\n",
        "   #testset\r\n",
        "    print(\"in test\")\r\n",
        "    lena = len(np.unique(actual))\r\n",
        "    lenp = len(np.unique(prediction))\r\n",
        "    if (lena>lenp):\r\n",
        "      n = lena\r\n",
        "    else:\r\n",
        "      n= lenp   \r\n",
        "    \r\n",
        "\r\n",
        "    if (iCount%cvCount == 0):\r\n",
        "      \r\n",
        "      fitNum =fitNum + 1\r\n",
        "      print(\"-----------------------------------------------------------\",fitNum,\"--------------------------------------------------------\")\r\n",
        "      \r\n",
        "       \r\n",
        "    \r\n",
        "    myScore.clear()\r\n",
        "    \r\n",
        "    for i in range(0, n):\r\n",
        "      cls=i\r\n",
        "      oneVall=np.array(list(map(oneVsAll, prediction)))\r\n",
        "      #print(len(np.unique(actual)))\r\n",
        "      \r\n",
        "      m[0].append(accuracy_score(actual,oneVall)*100)\r\n",
        "      m[1].append(precision_score(actual,oneVall))\r\n",
        "      m[2].append(recall_score(actual,oneVall))\r\n",
        "      m[3].append(f1_score(actual,oneVall))\r\n",
        "      m[4].append(roc_auc_score(actual,oneVall))\r\n",
        "      \r\n",
        "    \r\n",
        "    i_index = m[0].index(max(m[0]))  \r\n",
        "    #print(\"Index--------\",i_index)\r\n",
        "    myScore.append(m[0][i_index])\r\n",
        "    myScore.append(m[1][i_index])\r\n",
        "    myScore.append(m[2][i_index])\r\n",
        "    myScore.append(m[3][i_index])\r\n",
        "    myScore.append(m[4][i_index])\r\n",
        "\r\n",
        "    last = 0\r\n",
        "    #print(\"accurcy max---\",myScore[0])\r\n",
        "  return myScore[0]\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "def prec_score(estimator,x,y):\r\n",
        "  return myScore[1]\r\n",
        "\r\n",
        "def rec_score(estimator,x,y):\r\n",
        "  return myScore[2]\r\n",
        "\r\n",
        "def f1_s_score(estimator,x,y):\r\n",
        "  return myScore[3]\r\n",
        "\r\n",
        "def roc_score(estimator,x,y):\r\n",
        "  return myScore[4]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "loadData()\r\n",
        "\r\n",
        "\r\n",
        "#addOtherLabel()\r\n",
        "removeDuplicate()\r\n",
        "#balanceClass()\r\n",
        "print(df_train.groupby('label')['label'].count())\r\n",
        "splitData()\r\n",
        "\r\n",
        "X_train, X_test = one_hot_encode(X_train,X_test)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "scalers_to_test = [StandardScaler()]#\r\n",
        " \r\n",
        "params = {'scaler': scalers_to_test,\r\n",
        "          'reduce_dim': [None,PCA(n_components=2),PCA(n_components=3)],\r\n",
        "          'regressor__n_clusters': [10],\r\n",
        "          'regressor__random_state':[2000,100000]}\r\n",
        "          \r\n",
        "\"\"\"\r\n",
        "....................................or (both result same)\r\n",
        "params = [{'scaler': scalers_to_test,\r\n",
        "          'reduce_dim': [PCA()],\r\n",
        "          'reduce_dim__n_components': [5,20,40,55],\r\n",
        "          'regressor__n_clusters': [2,5],\r\n",
        "          'regressor__random_state':[0,2000,10000,50000,100000]}\r\n",
        "          ,\r\n",
        "          {\r\n",
        "          'scaler': scalers_to_test,\r\n",
        "          'reduce_dim': [None],\r\n",
        "          'regressor__n_clusters': [2,5],\r\n",
        "          'regressor__random_state':[0,2000,10000,50000,100000]    \r\n",
        "          }]\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "pipe = Pipeline(steps=[('scaler', StandardScaler()),('reduce_dim', PCA()),('regressor', KMeans())],verbose=2)\r\n",
        "\r\n",
        "\r\n",
        "scoring = {'accuracy': make_scorer(acc_score,greater_is_better=True,method='acc'),\r\n",
        "           'prec': prec_score,\r\n",
        "           'rec': rec_score,\r\n",
        "           'f1_s': f1_s_score,\r\n",
        "            'roc': roc_score}          \r\n",
        "\r\n",
        "grid = GridSearchCV(pipe, cv=cvCount, scoring=scoring,param_grid= params,return_train_score=trainScore,refit='accuracy',n_jobs=1, verbose=2)#\r\n",
        "\r\n",
        "grid.fit(X_train,Y_train)\r\n",
        "\r\n",
        "cvResult = pd.DataFrame(grid.cv_results_)\r\n",
        "#rmtree(cachedir)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Duplicates train: 67601\n",
            "0\n",
            "Duplicates test: 26387\n",
            "0\n",
            "label\n",
            "0    51890\n",
            "1    55850\n",
            "Name: label, dtype: int64\n",
            "X_train shape : (107740, 194)\n",
            "X_test shape : (55940, 194)\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "[CV] reduce_dim=None, regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   0.0s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=  13.0s\n",
            "in test\n",
            "----------------------------------------------------------- 1 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=None, regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=  14.5s\n",
            "[CV] reduce_dim=None, regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   0.0s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=  12.5s\n",
            "in test\n",
            "----------------------------------------------------------- 2 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=None, regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=  13.7s\n",
            "[CV] reduce_dim=None, regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   0.0s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=  12.3s\n",
            "in test\n",
            "----------------------------------------------------------- 3 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=None, regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=  13.6s\n",
            "[CV] reduce_dim=None, regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   0.0s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=  13.0s\n",
            "in test\n",
            "----------------------------------------------------------- 4 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=None, regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=  14.3s\n",
            "[CV] reduce_dim=None, regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   0.0s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=  14.3s\n",
            "in test\n",
            "----------------------------------------------------------- 5 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=None, regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=  15.6s\n",
            "[CV] reduce_dim=None, regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   0.0s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=  12.9s\n",
            "in test\n",
            "----------------------------------------------------------- 6 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=None, regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=  14.1s\n",
            "[CV] reduce_dim=None, regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   0.0s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=  13.3s\n",
            "in test\n",
            "----------------------------------------------------------- 7 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=None, regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=  14.6s\n",
            "[CV] reduce_dim=None, regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   0.0s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=  14.6s\n",
            "in test\n",
            "----------------------------------------------------------- 8 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=None, regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=  15.9s\n",
            "[CV] reduce_dim=None, regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   0.0s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=  12.2s\n",
            "in test\n",
            "----------------------------------------------------------- 9 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=None, regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=  13.5s\n",
            "[CV] reduce_dim=None, regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   0.0s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=  12.5s\n",
            "in test\n",
            "----------------------------------------------------------- 10 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=None, regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=  13.7s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.7s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   1.9s\n",
            "in test\n",
            "----------------------------------------------------------- 11 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   4.8s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.5s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   2.2s\n",
            "in test\n",
            "----------------------------------------------------------- 12 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   5.0s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.6s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   2.5s\n",
            "in test\n",
            "----------------------------------------------------------- 13 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   5.4s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.6s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   2.1s\n",
            "in test\n",
            "----------------------------------------------------------- 14 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   5.0s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.6s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   2.3s\n",
            "in test\n",
            "----------------------------------------------------------- 15 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   5.2s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.6s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   2.4s\n",
            "in test\n",
            "----------------------------------------------------------- 16 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   5.3s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.6s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   2.2s\n",
            "in test\n",
            "----------------------------------------------------------- 17 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   5.0s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.6s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   2.3s\n",
            "in test\n",
            "----------------------------------------------------------- 18 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   5.2s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.6s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   2.1s\n",
            "in test\n",
            "----------------------------------------------------------- 19 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   4.9s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.6s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   2.0s\n",
            "in test\n",
            "----------------------------------------------------------- 20 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   4.8s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.7s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   3.6s\n",
            "in test\n",
            "----------------------------------------------------------- 21 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   6.6s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.7s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   3.0s\n",
            "in test\n",
            "----------------------------------------------------------- 22 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   6.0s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.7s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   2.9s\n",
            "in test\n",
            "----------------------------------------------------------- 23 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   5.9s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.7s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   2.7s\n",
            "in test\n",
            "----------------------------------------------------------- 24 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   5.7s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.7s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   3.1s\n",
            "in test\n",
            "----------------------------------------------------------- 25 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=2000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   6.1s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.7s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   3.0s\n",
            "in test\n",
            "----------------------------------------------------------- 26 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   6.0s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.7s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   3.0s\n",
            "in test\n",
            "----------------------------------------------------------- 27 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   5.9s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.8s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   2.4s\n",
            "in test\n",
            "----------------------------------------------------------- 28 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   5.4s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.7s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   3.1s\n",
            "in test\n",
            "----------------------------------------------------------- 29 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   6.1s\n",
            "[CV] reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True) \n",
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.4s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   1.7s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=   2.9s\n",
            "in test\n",
            "----------------------------------------------------------- 30 --------------------------------------------------------\n",
            "in train\n",
            "[CV]  reduce_dim=PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), regressor__n_clusters=10, regressor__random_state=100000, scaler=StandardScaler(copy=True, with_mean=True, with_std=True), total=   5.9s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  4.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.5s\n",
            "[Pipeline] ........ (step 2 of 3) Processing reduce_dim, total=   0.0s\n",
            "[Pipeline] ......... (step 3 of 3) Processing regressor, total=  14.3s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "9ZJ1D87oeA2_",
        "outputId": "ef12ecc2-f841-4726-e3b8-e8c7d86a1a84"
      },
      "source": [
        "cc = ['param_reduce_dim','param_regressor__n_clusters','param_regressor__random_state','param_scaler','params','mean_test_accuracy','rank_test_accuracy','mean_test_prec','rank_test_prec','mean_test_rec','rank_test_rec','mean_test_f1_s','rank_test_f1_s','mean_test_roc','rank_test_roc']\r\n",
        "s = cvResult[cc]\r\n",
        "s.sort_values(['rank_test_accuracy'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>param_reduce_dim</th>\n",
              "      <th>param_regressor__n_clusters</th>\n",
              "      <th>param_regressor__random_state</th>\n",
              "      <th>param_scaler</th>\n",
              "      <th>params</th>\n",
              "      <th>mean_test_accuracy</th>\n",
              "      <th>rank_test_accuracy</th>\n",
              "      <th>mean_test_prec</th>\n",
              "      <th>rank_test_prec</th>\n",
              "      <th>mean_test_rec</th>\n",
              "      <th>rank_test_rec</th>\n",
              "      <th>mean_test_f1_s</th>\n",
              "      <th>rank_test_f1_s</th>\n",
              "      <th>mean_test_roc</th>\n",
              "      <th>rank_test_roc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>None</td>\n",
              "      <td>10</td>\n",
              "      <td>2000</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'reduce_dim': None, 'regressor__n_clusters': ...</td>\n",
              "      <td>74.849638</td>\n",
              "      <td>1</td>\n",
              "      <td>0.675090</td>\n",
              "      <td>2</td>\n",
              "      <td>0.989124</td>\n",
              "      <td>2</td>\n",
              "      <td>0.801897</td>\n",
              "      <td>1</td>\n",
              "      <td>0.738767</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PCA(copy=True, iterated_power='auto', n_compon...</td>\n",
              "      <td>10</td>\n",
              "      <td>2000</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'reduce_dim': PCA(copy=True, iterated_power='...</td>\n",
              "      <td>74.773529</td>\n",
              "      <td>2</td>\n",
              "      <td>0.676860</td>\n",
              "      <td>1</td>\n",
              "      <td>0.976999</td>\n",
              "      <td>5</td>\n",
              "      <td>0.799454</td>\n",
              "      <td>2</td>\n",
              "      <td>0.737454</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>PCA(copy=True, iterated_power='auto', n_compon...</td>\n",
              "      <td>10</td>\n",
              "      <td>100000</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'reduce_dim': PCA(copy=True, iterated_power='...</td>\n",
              "      <td>74.068127</td>\n",
              "      <td>3</td>\n",
              "      <td>0.671424</td>\n",
              "      <td>3</td>\n",
              "      <td>0.972708</td>\n",
              "      <td>6</td>\n",
              "      <td>0.794189</td>\n",
              "      <td>3</td>\n",
              "      <td>0.730448</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>None</td>\n",
              "      <td>10</td>\n",
              "      <td>100000</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'reduce_dim': None, 'regressor__n_clusters': ...</td>\n",
              "      <td>72.151476</td>\n",
              "      <td>4</td>\n",
              "      <td>0.653728</td>\n",
              "      <td>4</td>\n",
              "      <td>0.989636</td>\n",
              "      <td>1</td>\n",
              "      <td>0.785990</td>\n",
              "      <td>4</td>\n",
              "      <td>0.711875</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PCA(copy=True, iterated_power='auto', n_compon...</td>\n",
              "      <td>10</td>\n",
              "      <td>2000</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'reduce_dim': PCA(copy=True, iterated_power='...</td>\n",
              "      <td>68.118619</td>\n",
              "      <td>5</td>\n",
              "      <td>0.620212</td>\n",
              "      <td>5</td>\n",
              "      <td>0.986023</td>\n",
              "      <td>4</td>\n",
              "      <td>0.760945</td>\n",
              "      <td>5</td>\n",
              "      <td>0.668665</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PCA(copy=True, iterated_power='auto', n_compon...</td>\n",
              "      <td>10</td>\n",
              "      <td>100000</td>\n",
              "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
              "      <td>{'reduce_dim': PCA(copy=True, iterated_power='...</td>\n",
              "      <td>68.046222</td>\n",
              "      <td>6</td>\n",
              "      <td>0.619620</td>\n",
              "      <td>6</td>\n",
              "      <td>0.986375</td>\n",
              "      <td>3</td>\n",
              "      <td>0.760600</td>\n",
              "      <td>6</td>\n",
              "      <td>0.667914</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    param_reduce_dim  ... rank_test_roc\n",
              "0                                               None  ...             1\n",
              "4  PCA(copy=True, iterated_power='auto', n_compon...  ...             2\n",
              "5  PCA(copy=True, iterated_power='auto', n_compon...  ...             3\n",
              "1                                               None  ...             4\n",
              "2  PCA(copy=True, iterated_power='auto', n_compon...  ...             5\n",
              "3  PCA(copy=True, iterated_power='auto', n_compon...  ...             6\n",
              "\n",
              "[6 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSCz6r7xx_rx",
        "outputId": "12637ca0-ca7d-42e9-c65f-4e54e7721c3e"
      },
      "source": [
        "nu = 5\r\n",
        "\r\n",
        "print(grid.cv_results_['param_reduce_dim'][nu])\r\n",
        "print(grid.cv_results_['params'][nu])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False)\n",
            "{'reduce_dim': PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
            "    svd_solver='auto', tol=0.0, whiten=False), 'regressor__n_clusters': 10, 'regressor__random_state': 100000, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dxi92DeV1Ms",
        "outputId": "3304c0ad-c8af-4446-89c5-1667fc19a18c"
      },
      "source": [
        "print(\"Kmean \\n\\n\")\r\n",
        "loadData()\r\n",
        "#df_train.columns\r\n",
        "#print(df_train.groupby('attack_cat')['attack_cat'].count())\r\n",
        "\r\n",
        "#print(df_train.groupby('label')['label'].count())\r\n",
        "#addOtherLabel()\r\n",
        "removeDuplicate()\r\n",
        "#balanceClass()\r\n",
        "splitData()\r\n",
        "\r\n",
        "\r\n",
        "X_train, X_test = one_hot_encode(X_train,X_test)\r\n",
        "\r\n",
        "X_train, X_test = scaling(X_train,X_test,StandardScaler())\r\n",
        "X_train,X_test = runPCA(X_train,X_test,3)\r\n",
        "\r\n",
        "kmean= KMeans(n_clusters=10,random_state=100000)\r\n",
        "kmean.fit(X_train)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kmean \n",
            "\n",
            "\n",
            "Duplicates train: 67601\n",
            "0\n",
            "Duplicates test: 26387\n",
            "0\n",
            "X_train shape : (107740, 194)\n",
            "X_test shape : (55940, 194)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
              "       n_clusters=10, n_init=10, n_jobs=None, precompute_distances='auto',\n",
              "       random_state=100000, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pNBmOYMqnzh"
      },
      "source": [
        "predictedTrain = kmean.predict(X_train)\r\n",
        "predictedTest= kmean.predict(X_test)\r\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk_SDyJTqKQ5",
        "outputId": "ea134118-9da7-4495-b2c2-bdfb3f265405"
      },
      "source": [
        "oneVsAllScore(10,predictedTrain, predictedTest)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    0.756432151475775\n",
            "1    0.5024132170038983\n",
            "2    0.3706237237794691\n",
            "3    0.4162149619454242\n",
            "4    0.5031557453127901\n",
            "5    0.518099127529237\n",
            "6    0.6145721180620011\n",
            "7    0.41170410246890665\n",
            "8    0.5241692964544273\n",
            "9    0.529636161128643\n",
            "Accuracy Train:  75.64321514757751\n",
            "Accuracy Test:  58.65212727922774\n",
            "Test set precision : 0.4841\n",
            "Test set recall    : 0.9740\n",
            "Test set F1-score  : 0.6467\n",
            "Test set Roc       : 0.6571\n",
            "True Positives  : 0.97\n",
            "True Negatives  : 0.34\n",
            "False Positives : 0.66\n",
            "False Negatives : 0.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eo48zcrJD2W"
      },
      "source": [
        "#.......................................................................Kmeans debug & testing\r\n",
        "trainScore = True\r\n",
        "cvCount = 5\r\n",
        "\r\n",
        "last=1 #test=0\r\n",
        "i_index=-1\r\n",
        "myScore=[]\r\n",
        "iCount=0\r\n",
        "fitNum=0\r\n",
        "\r\n",
        "\r\n",
        "def acc_score(actual,prediction,method):\r\n",
        "  global last\r\n",
        "  global i_index\r\n",
        "  global myScore\r\n",
        "  global iCount\r\n",
        "  global fitNum\r\n",
        " \r\n",
        "\r\n",
        "  m=[[],[],[],[],[]]\r\n",
        "  cls = -1\r\n",
        "  \r\n",
        "  \r\n",
        "  def oneVsAll(num):\r\n",
        "    if (num ==cls):\r\n",
        "      return 0\r\n",
        "    else:\r\n",
        "      return 1\r\n",
        "\r\n",
        "  if (last == 0 and trainScore):\r\n",
        "    #trainset\r\n",
        "    print(\"in train\")\r\n",
        "    pp = pd.DataFrame(actual)\r\n",
        "    ppp = pd.DataFrame(prediction)\r\n",
        "    print(\"actual : \",pp.groupby('label')['label'].count())\r\n",
        "    print(\"predicted : \",ppp.groupby(0)[0].count())\r\n",
        "    myScore.clear()\r\n",
        "    cls = i_index\r\n",
        "    #print(\"Index--------\",i_index)\r\n",
        "    oneVtest =np.array(list(map(oneVsAll, prediction)))\r\n",
        "    myScore.append(accuracy_score(actual,oneVtest)*100)\r\n",
        "    myScore.append(precision_score(actual,oneVtest))\r\n",
        "    myScore.append(recall_score(actual,oneVtest))\r\n",
        "    myScore.append(f1_score(actual,oneVtest))\r\n",
        "    myScore.append(roc_auc_score(actual,oneVtest))\r\n",
        "\r\n",
        "    last = 1\r\n",
        "    \r\n",
        "  else:\r\n",
        "   #testset\r\n",
        "    print(\"in test\")\r\n",
        "    lena = len(np.unique(actual))\r\n",
        "    lenp = len(np.unique(prediction))\r\n",
        "    if (lena>lenb):\r\n",
        "      n = lena\r\n",
        "    else:\r\n",
        "      n= lenb \r\n",
        "    if (iCount%cvCount == 0):\r\n",
        "      \r\n",
        "      fitNum =fitNum + 1\r\n",
        "      print(\"-----------------------------------------------------------\",fitNum,\"--------------------------------------------------------\")\r\n",
        "      \r\n",
        "    pp = pd.DataFrame(actual)\r\n",
        "    ppp = pd.DataFrame(prediction)\r\n",
        "    print(\"actual : \",pp.groupby('label')['label'].count())\r\n",
        "    print(\"predicted : \",ppp.groupby(0)[0].count())   \r\n",
        "    \r\n",
        "    myScore.clear()\r\n",
        "    if ((len(np.unique(actual))<2) or (len(np.unique(prediction))<2)):\r\n",
        "      print(\"############################################################################################################################\")\r\n",
        "    for i in range(0, n):\r\n",
        "      cls=i\r\n",
        "      oneVall=np.array(list(map(oneVsAll, prediction)))\r\n",
        "      #print(len(np.unique(actual)))\r\n",
        "      pppp = pd.DataFrame(oneVall)\r\n",
        "      print(\"onevs : \",pppp.groupby(0)[0].count())\r\n",
        "      #print(len(np.unique(oneVall)))\r\n",
        "      #print(\"accurcy---\",i,\"  \",accuracy_score(actual,oneVall)*100)\r\n",
        "      m[0].append(accuracy_score(actual,oneVall)*100)\r\n",
        "      m[1].append(precision_score(actual,oneVall))\r\n",
        "      m[2].append(recall_score(actual,oneVall))\r\n",
        "      m[3].append(f1_score(actual,oneVall))\r\n",
        "      m[4].append(roc_auc_score(actual,oneVall))\r\n",
        "      \r\n",
        "    \r\n",
        "    i_index = m[0].index(max(m[0]))  \r\n",
        "    #print(\"Index--------\",i_index)\r\n",
        "    myScore.append(m[0][i_index])\r\n",
        "    myScore.append(m[1][i_index])\r\n",
        "    myScore.append(m[2][i_index])\r\n",
        "    myScore.append(m[3][i_index])\r\n",
        "    myScore.append(m[4][i_index])\r\n",
        "\r\n",
        "    last = 0\r\n",
        "    #print(\"accurcy max---\",myScore[0])\r\n",
        "  return myScore[0]\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "def prec_score(estimator,x,y):\r\n",
        "  return myScore[1]\r\n",
        "\r\n",
        "def rec_score(estimator,x,y):\r\n",
        "  return myScore[2]\r\n",
        "\r\n",
        "def f1_s_score(estimator,x,y):\r\n",
        "  return myScore[3]\r\n",
        "\r\n",
        "def roc_score(estimator,x,y):\r\n",
        "  return myScore[4]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "loadData()\r\n",
        "\r\n",
        "\r\n",
        "#addOtherLabel()\r\n",
        "removeDuplicate()\r\n",
        "#balanceClass()\r\n",
        "#print(df_train.groupby('label')['label'].count())\r\n",
        "splitData()\r\n",
        "\r\n",
        "X_train, X_test = one_hot_encode(X_train,X_test)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "scalers_to_test = [MinMaxScaler(),None]#\r\n",
        " \r\n",
        "params = {'scaler': scalers_to_test,\r\n",
        "          'reduce_dim': [None,PCA(n_components=5)],\r\n",
        "          'regressor__n_clusters': [2],\r\n",
        "          'regressor__random_state':[0]}\r\n",
        "          \r\n",
        "\"\"\"\r\n",
        "....................................or (both result same)\r\n",
        "params = [{'scaler': scalers_to_test,\r\n",
        "          'reduce_dim': [PCA()],\r\n",
        "          'reduce_dim__n_components': [5,20,40,55],\r\n",
        "          'regressor__n_clusters': [2,5],\r\n",
        "          'regressor__random_state':[0,2000,10000,50000,100000]}\r\n",
        "          ,\r\n",
        "          {\r\n",
        "          'scaler': scalers_to_test,\r\n",
        "          'reduce_dim': [None],\r\n",
        "          'regressor__n_clusters': [2,5],\r\n",
        "          'regressor__random_state':[0,2000,10000,50000,100000]    \r\n",
        "          }]\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "pipe = Pipeline(steps=[('scaler', StandardScaler()),('reduce_dim', PCA()),('regressor', KMeans())],verbose=2)\r\n",
        "\r\n",
        "\r\n",
        "scoring = {'accuracy': make_scorer(acc_score,greater_is_better=True,method='acc'),\r\n",
        "           'prec': prec_score,\r\n",
        "           'rec': rec_score,\r\n",
        "           'f1_s': f1_s_score,\r\n",
        "            'roc': roc_score}    \r\n",
        "\r\n",
        "\r\n",
        "\"\"\"cv= [(slice(None), slice(None))]# for non crossvalidation\r\n",
        "cv1 = ShuffleSplit(test_size=0.20, n_splits=1, random_state=0)#only use 1 cvset as 0.2 of training set\r\n",
        "\"\"\"\r\n",
        "grid = GridSearchCV(pipe, cv=cvCount, scoring=scoring,param_grid= params,return_train_score=trainScore,refit=False,n_jobs=1, verbose=2)#\r\n",
        "grid.fit(X_train,Y_train)\r\n",
        "\r\n",
        "cvResult = pd.DataFrame(grid.cv_results_)\r\n",
        "cvResult\r\n",
        "#rmtree(cachedir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2o7M1Jnx_oc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsTlAER9x_lz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ0eoYFQx_gh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNlIf961RCoq"
      },
      "source": [
        "print(\".......................................................................GMM.............\")\r\n",
        "trainScore = True\r\n",
        "cvCount = 5\r\n",
        "\r\n",
        "last=1 #test=0\r\n",
        "i_index=-1\r\n",
        "myScore=[]\r\n",
        "iCount=0\r\n",
        "fitNum=0\r\n",
        "\r\n",
        "\r\n",
        "def acc_score(actual,prediction,method):\r\n",
        "  global last\r\n",
        "  global i_index\r\n",
        "  global myScore\r\n",
        "  global iCount\r\n",
        "  global fitNum\r\n",
        " \r\n",
        "\r\n",
        "  m=[[],[],[],[],[]]\r\n",
        "  cls = -1\r\n",
        "  \r\n",
        "  \r\n",
        "  def oneVsAll(num):\r\n",
        "    if (num ==cls):\r\n",
        "      return 0\r\n",
        "    else:\r\n",
        "      return 1\r\n",
        "\r\n",
        "  if (last == 0 and trainScore):\r\n",
        "     #trainset\r\n",
        "    print(\"in train\")\r\n",
        "    \r\n",
        "    myScore.clear()\r\n",
        "    cls = i_index\r\n",
        "    #print(\"Index--------\",i_index)\r\n",
        "    oneVtest =np.array(list(map(oneVsAll, prediction)))\r\n",
        "    myScore.append(accuracy_score(actual,oneVtest)*100)\r\n",
        "    myScore.append(precision_score(actual,oneVtest))\r\n",
        "    myScore.append(recall_score(actual,oneVtest))\r\n",
        "    myScore.append(f1_score(actual,oneVtest))\r\n",
        "    myScore.append(roc_auc_score(actual,oneVtest))\r\n",
        "\r\n",
        "    last = 1\r\n",
        "    \r\n",
        "  else:\r\n",
        "   #testset\r\n",
        "    print(\"in test\")\r\n",
        "    lena = len(np.unique(actual))\r\n",
        "    lenp = len(np.unique(prediction))\r\n",
        "    if (lena>lenp):\r\n",
        "      n = lena\r\n",
        "    else:\r\n",
        "      n= lenp\r\n",
        "    if (iCount%cvCount == 0):\r\n",
        "      \r\n",
        "      fitNum =fitNum + 1\r\n",
        "      print(\"-----------------------------------------------------------\",fitNum,\"--------------------------------------------------------\")\r\n",
        "\r\n",
        "    myScore.clear()\r\n",
        "    \r\n",
        "    for i in range(0, n):\r\n",
        "      cls=i\r\n",
        "      oneVall=np.array(list(map(oneVsAll, prediction)))\r\n",
        "      #print(len(np.unique(actual)))\r\n",
        "      #pppp = pd.DataFrame(oneVall)\r\n",
        "      #print(\"onevs : \",pppp.groupby(0)[0].count())\r\n",
        "      #print(len(np.unique(oneVall)))\r\n",
        "      #print(\"accurcy---\",i,\"  \",accuracy_score(actual,oneVall)*100)\r\n",
        "      m[0].append(accuracy_score(actual,oneVall)*100)\r\n",
        "      m[1].append(precision_score(actual,oneVall))\r\n",
        "      m[2].append(recall_score(actual,oneVall))\r\n",
        "      m[3].append(f1_score(actual,oneVall))\r\n",
        "      m[4].append(roc_auc_score(actual,oneVall))\r\n",
        "      \r\n",
        "    \r\n",
        "    i_index = m[0].index(max(m[0]))  \r\n",
        "    #print(\"Index--------\",i_index)\r\n",
        "    myScore.append(m[0][i_index])\r\n",
        "    myScore.append(m[1][i_index])\r\n",
        "    myScore.append(m[2][i_index])\r\n",
        "    myScore.append(m[3][i_index])\r\n",
        "    myScore.append(m[4][i_index])\r\n",
        "\r\n",
        "    last = 0\r\n",
        "    #print(\"accurcy max---\",myScore[0])\r\n",
        "  return myScore[0]\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "def prec_score(estimator,x,y):\r\n",
        "  return myScore[1]\r\n",
        "\r\n",
        "def rec_score(estimator,x,y):\r\n",
        "  return myScore[2]\r\n",
        "\r\n",
        "def f1_s_score(estimator,x,y):\r\n",
        "  return myScore[3]\r\n",
        "\r\n",
        "def roc_score(estimator,x,y):\r\n",
        "  return myScore[4]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "loadData()\r\n",
        "\r\n",
        "#print(df_train.groupby('label')['label'].count())\r\n",
        "#addOtherLabel()\r\n",
        "removeDuplicate()\r\n",
        "#balanceClass()\r\n",
        "splitData()\r\n",
        "\r\n",
        "X_train, X_test = one_hot_encode(X_train,X_test)\r\n",
        "\r\n",
        "\r\n",
        "cv_types = [ 'tied']\r\n",
        "scalers_to_test = [StandardScaler(), MinMaxScaler(),None]\r\n",
        " \r\n",
        "params = {'scaler': scalers_to_test,\r\n",
        "          'reduce_dim': [None,PCA(n_components=5),PCA(n_components=5)],\r\n",
        "          'regressor__n_components': [10],\r\n",
        "          'regressor__covariance_type': cv_types,\r\n",
        "          'regressor__random_state':[0]}\r\n",
        "          \r\n",
        "\"\"\"\r\n",
        "....................................or (both result same in reduce__dim)\r\n",
        "params = [{'scaler': scalers_to_test,\r\n",
        "          'reduce_dim': [PCA()],\r\n",
        "          'reduce_dim__n_components': [5,20,40,55],\r\n",
        "          'regressor__n_clusters': [2,5],\r\n",
        "          'regressor__random_state':[0,2000,10000,50000,100000]}\r\n",
        "          ,\r\n",
        "          {\r\n",
        "          'scaler': scalers_to_test,\r\n",
        "          'reduce_dim': [None],\r\n",
        "          'regressor__n_clusters': [2,5],\r\n",
        "          'regressor__random_state':[0,2000,10000,50000,100000]    \r\n",
        "          }]\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "pipe = Pipeline(steps=[('scaler', StandardScaler()),('reduce_dim', PCA()),('regressor', GaussianMixture())],verbose=2)\r\n",
        "\r\n",
        "\r\n",
        "scoring = {'accuracy': make_scorer(acc_score,greater_is_better=True,method='acc'),\r\n",
        "           'prec': prec_score,\r\n",
        "           'rec': rec_score,\r\n",
        "           'f1_s': f1_s_score,\r\n",
        "            'roc': roc_score}          \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "grid = GridSearchCV(pipe, cv=cvCount, scoring=scoring,param_grid= params,return_train_score=trainScore,refit='accuracy',n_jobs=1, verbose=2)#\r\n",
        "\r\n",
        "grid.fit(X_train,Y_train)\r\n",
        "\r\n",
        "cvResult = pd.DataFrame(grid.cv_results_)\r\n",
        "\r\n",
        "#rmtree(cachedir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EUQ5wl5x_dW"
      },
      "source": [
        "cvResult.sort_values(['rank_test_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ_DySnWx_U0"
      },
      "source": [
        "cc = ['param_reduce_dim','param_regressor__covariance_type', 'param_regressor__n_components','param_regressor__random_state','param_scaler','params','mean_test_accuracy','rank_test_accuracy','mean_test_prec','rank_test_prec','mean_test_rec','rank_test_rec','mean_test_f1_s','rank_test_f1_s','mean_test_roc','rank_test_roc']\r\n",
        "s = cvResult[cc]\r\n",
        "s.sort_values(['rank_test_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqRps1wn3lpg"
      },
      "source": [
        "nu = 10\r\n",
        "\r\n",
        "print(grid.cv_results_['param_reduce_dim'][nu])\r\n",
        "print(grid.cv_results_['params'][nu])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI0H5Z4F4048"
      },
      "source": [
        "print(\"GMM \\n\\n\")\r\n",
        "loadData()\r\n",
        "#df_train.columns\r\n",
        "#print(df_train.groupby('attack_cat')['attack_cat'].count())\r\n",
        "\r\n",
        "#print(df_train.groupby('label')['label'].count())\r\n",
        "#addOtherLabel()\r\n",
        "removeDuplicate()\r\n",
        "#balanceClass()\r\n",
        "splitData()\r\n",
        "\r\n",
        "\r\n",
        "X_train, X_test = one_hot_encode(X_train,X_test)\r\n",
        "\r\n",
        "X_train, X_test = scaling(X_train,X_test,MinMaxScaler())\r\n",
        "X_train,X_test = runPCA(X_train,X_test,5)\r\n",
        "\r\n",
        "gmm= GaussianMixture(n_components=10,random_state=0,covariance_type='full')\r\n",
        "gmm.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p0olWZK402W"
      },
      "source": [
        "predictedTrain = gmm.predict(X_train)\r\n",
        "predictedTest= gmm.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngODZqD23llY"
      },
      "source": [
        "oneVsAllScore(10,predictedTrain, predictedTest)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}